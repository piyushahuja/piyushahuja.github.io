I"Fi<p>RL methods follow two general ideas:</p>
<ul>
  <li>Value Based: We could estimate how ‚Äúgood‚Äù a state or state/action pair is, and then we could have our agent behave greedily.</li>
  <li>Policy Based: We could find an optimal policy, a policy being a function that outputs an action for each state. 
These model and optimize the policy directly (rather than indirectly). The value of the reward (objective) function depends on this policy and then various algorithms can be applied to optimize for the best reward.</li>
</ul>

<hr />

<p>Policy based methods are better in the following settings:</p>
<ul>
  <li>There are situations where it is more efficient to represent policy than value functions. If the state space is continuous or too large, it would be difficult to store value functions.  In continuous action spaces* (e.g., robot control, trying to show the perfect advert): Value-based methods require finding the maximum, which can be prohibitively expensive.</li>
  <li>State aliasing: If you are only partially observing the environment, or your features limit your view of the world, then it can be optimal to use a stochastic policy, in which case policy search methods can be better than value methods.</li>
  <li>Value-based methods can oscillate or diverge. With policy gradients, you smoothly update your policy. You don‚Äôt get dramatic changes in what action you are going to take. You make incremental changes to policy in the direction that makes policy better.</li>
</ul>

<hr />
<p><strong>Policy Gradient Theorem</strong></p>

<p>The objective in RL is the expected return over trajectories:</p>

\[J(\theta) = E_{\tau \sim \pi_{\theta}} [R(\tau)]\]

<p>Where a trajectory \(\tau = (s_0, a_0, s_1, a_1, ..., s_T, a_T)\).</p>

<p>Policy gradient theorem (REINFORCE) gives:</p>

\[\nabla_{\theta} J(\theta) = E_{\tau \sim \pi_{\theta}} [R(\tau) \cdot \nabla_{\theta} \log \pi_{\theta}(\tau)]\]

<p>Factorizing the Trajectory Probability:</p>

\[\pi_{\theta}(\tau) = p(s_0) \prod_{t=0}^{T} \pi_{\theta}(a_t | s_t) p(s_{t+1} | s_t, a_t)\]

\[\nabla_{\theta} J(\theta) = E_{\tau} [R(\tau) \cdot \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t)]\]

<p>The per-step sum form:</p>

<p>\(R(\tau) \cdot \sum_{t} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t)\)
is computationally what we actually implement, because we compute the log-prob of each action as we sample the trajectory.</p>

<p><strong>Note</strong>
Credit assignment:</p>

<p>Instead of using the same R(œÑ) for all timesteps, we can refine credit assignment by using the return from each step onward:</p>

\[R_t = Œ£_{k=t}^{T} Œ≥^{k‚àít} * r_k\]

<p>Then the gradient becomes:</p>

\[‚àá_Œ∏ J(Œ∏) = E_{œÑ‚àºœÄ_Œ∏} [ Œ£_{t=0}^{T} R_t * ‚àá_Œ∏ log œÄ_Œ∏(a_t | s_t) ]\]

<p>The trajectory return  \($R(œÑ)\) and stepwise return  forms are mathematically equivalent in expectation</p>

<p>Using full  \(ùëÖ(ùúè)\) gives the same weight to every action that can high variance in long episodes. Using stepwise  gives more precise credit assignment to actions closer to the rewards. This is variance reduction but keeps the estimator unbiased.</p>

<p>The <strong>log trick</strong> (likelihood ratio trick): \(\nabla_{\theta} \pi_{\theta} = \pi_{\theta} \cdot \nabla_{\theta} \log \pi_{\theta}\) makes the math work cleanly with sampling.</p>

<p><strong>Assumptions:</strong></p>
<ol>
  <li>The policy is differentiable</li>
  <li>We know the gradient  (Gaussian, softmax, differentiable neural network etc.)</li>
</ol>

<p>We want to optimize expected return:</p>

\[\nabla_{\theta} J(\theta) = \nabla_{\theta} \mathbb{E}_{\tau} [R(\tau)] = \mathbb{E}_{\tau} \left[ R(\tau) \nabla_{\theta} \log p_{\theta}(\tau) \right]\]

<p>This pulls the gradient outside the sampling process.</p>

<p>The trajectory probability is a product of policy probabilities over time, the log of a product becomes a sum of logs. Only the policy depends on $\theta$, so we ignore the rest when computing gradients.</p>

\[\nabla_{\theta} J(\theta) = \mathbb{E}_{\tau} \left[ \sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \cdot R(\tau) \right]\]

<hr />

<p><strong>Non Differentiable Computation</strong></p>

<p>In the standard Reinforcement Learning (RL) process, the following steps occur:</p>

<p>The state at time \(t\), denoted as \(s_t\), is processed through the policy, denoted as \(œÄ_Œ∏\), to determine the action at time \(t\), denoted as \(a_t\). This action is then introduced to the environment, which in turn provides the new state at time \(t+1\), denoted as \(s_{t+1}\), and the reward at time \(t\), denoted as \(r_t\).</p>

<p>This can be mathematically represented as:</p>

\[s_t ‚Üí œÄ_Œ∏(a_t) ‚Üí Environment ‚Üí s_{t+1}, r_t\]

<p>Obtaining a reward in a trajectory in RL involves non-differentiable computation, even the policy is differentiable:</p>

<ul>
  <li>Action is sampled (discrete or stochastic). There is no gradient through sampling.</li>
  <li>The transition in the environment \(s_t, a_t ‚Ü¶ s_{t+1}\) is a black box, implying that we do not have knowledge how the environment selects state based on action.</li>
  <li>Similarly, the reward \(r_t = R(s_t, a_t)\) is also a black box, indicating that we do not have knowledge of the reward that the environment provides based on our action and the state. We cannot compute \(‚àÇR/‚àÇa\)</li>
</ul>

<p>We are unable to apply the chain rule from the reward back to \(Œ∏\) in a continuous manner because we cannot determine how the sampling or environmental reward changes with \(Œ∏\).</p>

<p>As a result, we consider the reward as an external scalar signal that influences a policy gradient.</p>

<hr />

<p>What if we could model the environment and reward function as differentiable?</p>

<p>If we could model the environment and reward function as differentiable (as in differentiable simulators)</p>

<p>\(f: s_{t+1} = f(s_t, a_t)\)
\(s_{t+1}\) is differentiable</p>

\[r_t = g(s_t, a_t)\]

<p>\(r_t\) is differentiable</p>

<p>Then you could skip policy gradient and just do end-to-end backpropagation:</p>

\[\nabla_{\theta} R = \sum_{t} \frac{\partial R}{\partial a_t} \frac{\partial a_t}{\partial \theta}\]

<p>This is much more sample-efficient because you use true gradients instead of REINFORCE‚Äôs noisy estimates.</p>

<hr />

<p><strong>Comparison with supervised learning</strong></p>

<p>Cross-entropy loss is really the negative log-likelihood of the correct label
Gradient of supervised learning is:</p>

\[\nabla_{\theta} L = -\nabla_{\theta} \log p_{\theta}(y|x)\]

<p>In supervised learning, we know the target label ‚Üí smooth differentiable loss.
In RL, the ‚Äúlabel‚Äù is implicit and stochastic via reward ‚Üí need expectation over trajectories</p>

<hr />

<h2 id="softmax-policy-for-discrete-actions">Softmax Policy for Discrete Actions</h2>

<p>Alternative to \(\epsilon\)-greedy:</p>

\[\pi_i = \frac{e^{z_i}}{\sum_j e^{z_j}}\]

\[\log(\pi_i) = z_i - \log\left(\sum_j e^{z_j}\right)\]

<p>The gradient with respect to each logit \(z_k\) is:</p>

\[\frac{\partial \log(\pi_i)}{\partial z_k} = 
\begin{cases} 
1 - \pi_i &amp; \text{if } k = i \\
-\pi_k &amp; \text{if } k \neq i 
\end{cases}\]

<p>In a linear softmax policy, logits are:</p>

\[z_a(s) = \theta_a^T s\]

\[\log(\pi_{\theta}(i|s)) = \theta_i^T s - \log\left(\sum_b e^{\theta_b^T s}\right)\]

<p>Derivative w.r.t parameters \(\theta_k\):</p>

\[\frac{\partial \log(\pi_{\theta}(i|s))}{\partial \theta_k} = 
\begin{cases} 
s \cdot (1 - \pi_{\theta}(i|s)), &amp; \text{if } k = i \\
-s \cdot \pi_{\theta}(k|s), &amp; \text{if } k \neq i 
\end{cases}\]

<p>For a single step with return \(G_t\) and learning rate \(\alpha\):</p>

<p>The update rule for the parameters \(\theta_k\) is given by:</p>

\[\theta_k \leftarrow \theta_k + \alpha \cdot G_t \cdot \nabla_{\theta_k} \log(\pi_{\theta}(a_t | s_t))\]

<p>This implies:</p>

<p>If the chosen action is \(k = a_t\):</p>

\[\theta_{a_t} \leftarrow \theta_{a_t} + \alpha \cdot G_t \cdot (1 - \pi_{\theta}(a_t | s_t)) \cdot s_t\]

<p>For other actions:</p>

\[\theta_k \leftarrow \theta_k - \alpha \cdot G_t \cdot \pi_{\theta}(k | s_t) \cdot s_t\]

<p>If a feature occurs and gets more reward, we want to adjust the policy to have more of that feature.</p>

<p><strong>Example</strong>: Gaussian policy for continuous actions.</p>

<hr />

<p>Why use a neural network (limitations if a linear model)
A single flat hyperplane divides state space into ‚Äúprefer action i‚Äù vs. ‚Äúprefer action j‚Äù If the environment is simple and separable, this works. But it cannot capture complex, non-linear decision regions
First layer warps the state space into a feature space. In feature space, the final decision is still linear ‚Üí hyperplanes in feature space When mapped back to the original state space, those hyperplanes become curved, flexible surfaces.</p>

<hr />

<p>Episode-based update (REINFORCE): High variance (needs normalization or baseline). Learning is slow if episodes are long</p>

<ul>
  <li>\(G_t\) is a single noisy Monte Carlo sample of the return.  Especially early on, a single high-return trajectory can push the policy too far, hurting generalization. Lucky or unlucky episodes can cause big policy swings. Risk of catastrophic forgetting caused by over-updating from a single trajectory.</li>
</ul>

<p>We can reduce variance by subtracting a baseline:</p>

<p>Instead of \(G_t\), use:</p>

\[A_t = G_t - b(s_t)\]

<p>\(b(s_t)\) is a baseline (\(V(s_t)\) in Actor-Critic)</p>

<p>All modern policy gradient methods (PPO, A2C, A3C, SAC) include: A critic, Advantage normalization, Or mini-batch updates
Step-based update (Actor-Critic):  Reduces variance because only deviations from expected return drive updates.  Smaller, more targeted updates, Less destabilizing global shifts in policy weights.  Lower variance, More sample-efficient.
A2C/A3C were breakthrough algorithms for: Continuous learning in long episodes</p>

<hr />

<h2 id="implementation">Implementation</h2>

<p>The function gives you discounted, normalized returns per step, ready for use in policy gradient loss calculation:
\(G_t = r_t + Œ≥*r_{t+1} + Œ≥^2*r_{t+2}\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_stepwise_returns</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">discount_factor</span><span class="p">):</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">rewards</span><span class="p">):</span>
         <span class="c1">#  if r != 0: R = 0 reset the sum, since this was a game boundary (pong specific)
</span>        <span class="n">R</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">R</span> <span class="o">*</span> <span class="n">discount_factor</span> 
        <span class="n">returns</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>
    <span class="n">normalized_returns</span> <span class="o">=</span> <span class="p">(</span><span class="n">returns</span> <span class="o">-</span> <span class="n">returns</span><span class="p">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">returns</span><span class="p">.</span><span class="n">std</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">normalized_returns</span>
</code></pre></div></div>
<p><strong>Example:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rewards</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">discount_factor</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="c1"># Raw returns: [2.458, 1.62, 1.8, 2]
# Discounted normalized returns: tensor([ 1.57, -1.13, -0.55,  0.11])
</span></code></pre></div></div>

<hr />

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="n">stepwise_returns</span><span class="p">,</span> <span class="n">log_prob_actions</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">stepwise_returns</span> <span class="o">*</span> <span class="n">log_prob_actions</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></div>

<p>We need a section that will collect one episode: sample actions</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward_pass</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">discount_factor</span><span class="p">):</span>
    <span class="n">log_prob_actions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">episode_return</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">policy</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">observation</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">observation</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">action_pred</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
        <span class="n">action_prob</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">action_pred</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">distributions</span><span class="p">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">action_prob</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="n">log_prob_action</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
        <span class="n">log_prob_actions</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_prob_action</span><span class="p">)</span>
        <span class="n">rewards</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
        <span class="n">episode_return</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">log_prob_actions</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">log_prob_actions</span><span class="p">)</span>
    <span class="n">stepwise_returns</span> <span class="o">=</span> <span class="n">calculate_stepwise_returns</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">discount_factor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">episode_return</span><span class="p">,</span> <span class="n">stepwise_returns</span><span class="p">,</span> <span class="n">log_prob_actions</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="n">action</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">output</span><span class="p">.</span><span class="n">item</span><span class="p">()</span> <span class="k">else</span> <span class="mi">3</span>  <span class="c1"># roll the dice!    
</span><span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">reward_sum</span> <span class="o">+=</span> <span class="n">reward</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">update_policy</span><span class="p">(</span><span class="n">stepwise_returns</span><span class="p">,</span> <span class="n">log_prob_actions</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">stepwise_returns</span> <span class="o">=</span> <span class="n">stepwise_returns</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">calculate_loss</span><span class="p">(</span><span class="n">stepwise_returns</span><span class="p">,</span> <span class="n">log_prob_actions</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">MAX_EPOCHS</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="n">DISCOUNT_FACTOR</span> <span class="o">=</span> <span class="mf">0.99</span>
    <span class="n">N_TRIALS</span> <span class="o">=</span> <span class="mi">25</span>
    <span class="n">REWARD_THRESHOLD</span> <span class="o">=</span> <span class="mi">475</span>
    <span class="n">PRINT_INTERVAL</span> <span class="o">=</span> <span class="mi">10</span>


    <span class="c1"># Defines network architecture parameters based on environment‚Äôs state and action spaces.
</span>    <span class="n">INPUT_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># Input dimension (state size)
</span>    <span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">128</span> <span class="c1"># Number of neurons in hidden layer
</span>    <span class="n">OUTPUT_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span> <span class="c1"># Number of actions (output dim)
</span>    <span class="n">DROPOUT</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># Dropout probability in policy network
</span>    <span class="n">policy</span> <span class="o">=</span> <span class="n">PolicyNetwork</span><span class="p">(</span><span class="n">INPUT_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">OUTPUT_DIM</span><span class="p">,</span> <span class="n">DROPOUT</span><span class="p">)</span>


    <span class="n">episode_returns</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">MAX_EPOCHS</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">episode_return</span><span class="p">,</span> <span class="n">stepwise_returns</span><span class="p">,</span> <span class="n">log_prob_actions</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">DISCOUNT_FACTOR</span><span class="p">)</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">update_policy</span><span class="p">(</span><span class="n">stepwise_returns</span><span class="p">,</span> <span class="n">log_prob_actions</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
        <span class="n">episode_returns</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_return</span><span class="p">)</span>
        <span class="n">mean_episode_return</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">episode_returns</span><span class="p">[</span><span class="o">-</span><span class="n">N_TRIALS</span><span class="p">:])</span>
        <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">PRINT_INTERVAL</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'| Episode: </span><span class="si">{</span><span class="n">episode</span><span class="p">:</span><span class="mi">3</span><span class="si">}</span><span class="s"> | Mean Rewards: </span><span class="si">{</span><span class="n">mean_episode_return</span><span class="p">:</span><span class="mf">5.1</span><span class="n">f</span><span class="si">}</span><span class="s"> |'</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mean_episode_return</span> <span class="o">&gt;=</span> <span class="n">REWARD_THRESHOLD</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Reached reward threshold in </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s"> episodes'</span><span class="p">)</span>
            <span class="k">break</span>

</code></pre></div></div>

<hr />

<p>Pong</p>

<p>This network will take the state of the game and decide what we should do (move UP or DOWN). As our favorite simple block of compute we‚Äôll use a 2-layer neural network that takes the raw image pixels (100,800 numbers total (210<em>160</em>3)), and produces a single number indicating the probability of going UP.</p>

<p>We can improve on this:</p>

<p>Equivalent to using trajectory-level reward only, no stepwise credit assignment</p>

<p>Each action gets credit proportional to its actual impact on reward (via discounted sum of future rewards), Avoids giving equal blame to irrelevant early actions in losing games</p>

<hr />

<hr />

<p>Resources:</p>

<ul>
  <li><a href="https://karpathy.github.io/2016/05/31/rl/">Pong by Karpathy</a></li>
  <li><a href="https://www.datacamp.com/tutorial/reinforcement-learning-with-gymnasium">CartPole</a></li>
  <li><a href="https://spinningup.openai.com/en/latest/spinningup/exercises.html#problem-set-1-basics-of-implementation">OpenAI Problem Sets</a></li>
</ul>

<hr />

:ET