I"IC<blockquote>
  <p>Learning from experience is about reducing surprise about what works.</p>
</blockquote>

<p>Log Probability seems to jump out everwhere in ML, not dissimilar to how $\pi$ comes up again and again in geometry.</p>

<p>You often want to differentiate an expectation in reinforcement learning:</p>

\[\nabla_{\theta} \mathbb{E}_{a \sim \pi_{\theta}(\cdot \mid s)}[f(a)]\]

<p>The gradient passes through the distribution $\pi_{\theta}$, so you get:</p>

\[\nabla_{\theta} \mathbb{E}_{\pi_{\theta}} [f(a)] = \mathbb{E}_{\pi_{\theta}} \left[ f(a) \nabla_{\theta} \log \pi_{\theta}(a \mid s) \right]\]

<p>This is the score function trick or log-derivative trick, and it allows you to estimate gradients without differentiating through the random sampling process</p>

<p>In Supervised Learning for classification, we minimize the negative log likelihood of the correct class (cross entropy loss):</p>

\[L = -\log p_\theta(y_{\text{true}} \mid x)\]

<p>In Diffusion Models, the training objective minimizes the KL divergence (difference of log-probabilities) between the forward and reverse processes.</p>

<p>The Kullback-Leibler (KL) divergence is</p>

\[D_{KL}(p \| q) = \mathbb{E}_{p}\left[ \log p(x) - \log q(x) \right]\]

<hr />

<p>Logic is absolute. The real world is messy. What would reasoning look like if we model the world as a dynamic entity where many possible worlds are possible, where events unfold, and as we observe what happens alternative realities are destroyed?  What laws of thought would govern reasoning about uncertainty and information?</p>

<p>When faced with uncertainty, instead of abandoning logic, we extend it through probability.  Probability is the study of correct reasoning under uncertainty.</p>

<p>If logic deals in truth values: 0 or 1, probability deals in degrees of plausibility: [0,1]. Instead of propositions, we study events and evidence.</p>

<p>So:</p>

<p>P(A) = 1 =&gt; I know A is true.</p>

<p>P(A) = 0 =&gt; I know A is false.</p>

<p>Intermediate values =&gt; â€œI donâ€™t know; A is more or less plausible.â€</p>

<p>Probabilities are thus generalized truth values.<sup id="fnref:cox" role="doc-noteref"><a href="#fn:cox" class="footnote" rel="footnote">1</a></sup></p>

<p>In logic, inference is deductive: If A implies B, and A is true, then B must be true.</p>

<p>In probability, Bayesian inference is inductive: If A supports B with likelihood, and A is plausible, then Bâ€™s plausibility updates by Bayesâ€™ rule.</p>

<p>A probabilistic model $p_\theta(x)$ encodes your beliefs about how likely different observations $x$ are, given parameters $\theta$. When you observe a new data point $x$, you want to update those beliefs.</p>

<p>Bayesâ€™ rule formalizes this:</p>

\[p(\theta \mid x) \propto p(x \mid \theta)\,p(\theta)\]

<p>Bayesian inference would treat  ğœƒ as uncertain and update a whole posterior.</p>

<hr />

<p>There are certain dynamics that play out when we model the world as events with uncertainty and reason about it:</p>

<ul>
  <li>For independent events, likelihoods are multiplicative. If one event is risky, and the other is risky, the risk of both occuring is muliplicative.</li>
</ul>

<p>Consider several examples of multiplicative probability in independent events:</p>

<ul>
  <li>
    <p><strong>Flipping 10 coins:</strong><br />
The probability of obtaining one specific sequence of heads and tails is<br />
\(\left(\frac{1}{2}\right)^{10}\) since each coin flip is independent with probability $\frac{1}{2}$ for each outcome.</p>
  </li>
  <li>
    <p><strong>Observing 100 i.i.d. data points:</strong><br />
The likelihood of observing a particular sequence is the product of the individual probabilities:
\(p(x_1, x_2, \dots, x_{100}) = \prod_{i=1}^{100} p(x_i)\)
provided the $x_i$ are independent and identically distributed.</p>
  </li>
  <li>
    <p><strong>Joint risk of independent failures:</strong><br />
Suppose a machine has a $\frac{1}{10}$ chance of failure each year.<br />
If you run two machines independently in parallel, the probability that both fail is:
\(\frac{1}{10} \times \frac{1}{10} = \frac{1}{100}\)</p>
  </li>
  <li>
    <p><strong>False positives in independent tests:</strong><br />
If the chance a suspect is guilty purely by random chance is $\frac{1}{100}$,<br />
and each of two independent tests has a $\frac{1}{10}$ false-positive rate,<br />
then the chance both tests indicate â€œguiltyâ€ by random chance is:
\(\frac{1}{10} \times \frac{1}{10} = \frac{1}{100}\)</p>
  </li>
</ul>

<!-- Joint occurrence of facts is modelled by intersection. It is the natural operation for â€œcompounding realityâ€.   When evidence accumulates, itâ€™s through conjunction: Data point 1 and Data point 2 and Data point 3. The likelihood of all those observations is the product of their individual likelihoods (under independence). 


The uncertainty of world-states is what ties possible worlds to probability.  

Possible Worlds: Each independent degree of freedom multiplies the number of possible worlds. With nnn binary independent factors, you donâ€™t get n+1n+1n+1 outcomes â€” you get 2n2^n2n. What compounds is the size of the state space of reality. 

Each new fact or discovered outcome slices away possible worlds. Learning both A and B means weâ€™re narrowing further, i.e. moving into a smaller intersection.  -->

<p>The real world is fundamentally uncertain. It consists of events that we observe. Once we observe an event, we get some information.</p>

<p>How much information? How do we measure it? How do we reason about it? How should information from an observation behave under the logic of uncertainty?</p>

<p>An event conveys information. The amount of information is not determined by its content, but by how surprising it was given our baseline expectation</p>

<p>Rare events give lots of information (big surprise). Common events give little information (small surprise). Impossible events give infinite information (youâ€™d have to revise your whole worldview).</p>

<p>Shannonâ€™s insight was to anchor information to how surprising an event is i.e. â€œhow unlikely is the world we observe, given our expectations?â€</p>

<p>Shannon reasoned:</p>

<p>Information should be a monotonic decreasing function of probability</p>

<p>If something is expected (probability close to 1), it gives you little new knowledge. Example: â€œThe sun rose this morning.â€ Low information. If something is rare (probability close to 0), itâ€™s very informative. Example: â€œThe stock market crashed today.â€ High information. Thus, the rarity (probability) of an event must set its information content. Higher ppp â†’ less surprise â†’ less information.</p>

<p>For independent events, information should add up.</p>

<p>If two events happen independently, the total information you get is the sum of information from each.</p>

<p>Suppose you want to know the outcome of two independent experiments:</p>

<p>Flip a fair coin $\left(P = \frac{1}{2}\right)$</p>

<p>Roll a fair die $\left(P = \frac{1}{6}\right)$</p>

<p>To identify both outcomes, youâ€™d need the same number of yes/no questions as asking them separately, then adding the answers. This means any measure of â€œinformationâ€ contained needs to be additive.</p>

<p>The only function that satisfies both properties is the logarithm:</p>

\[I(x) = -\log P(x)\]

<p>Log Probability is the measure of â€œinformationâ€.<sup id="fnref:bits" role="doc-noteref"><a href="#fn:bits" class="footnote" rel="footnote">2</a></sup><sup id="fnref:addition" role="doc-noteref"><a href="#fn:addition" class="footnote" rel="footnote">3</a></sup></p>

<!-- 
It ties prediction and control together Prediction: information tells you how much uncertainty was reduced by seeing the actual outcome. Control: information tells you how much â€œroom to maneuverâ€ you gain when you know what actually happened.


Itâ€™s operational If you want to transmit outcomes over a noisy channel, you need enough symbols to cover the worst-case surprise. That leads directly to coding theorems and compression limits. -->
<!-- 
Logic says: â€œIf X is true, then Y must follow.â€ Probability says: â€œGiven X, Y becomes more plausible.â€ Information says: â€œHow much more plausible did Y get after we actually observed X?â€ -->

<hr />

<p><strong>Gradient of Information/Log-likelihood gradients (Score functions)</strong></p>

<p>Once we observe an event and get some information, how would we learn from it? How would we update our belief?</p>

<p>Letâ€™s revisit our model:</p>

<p>Say a probabilistic model $p_\theta(x)$ encodes your beliefs about how likely different observations $x$ are, given parameters $\theta$.</p>

<p>Bayesâ€™ rule tells you how to update a full probability distribution after seeing data:</p>

<p>ğ‘
(
ğœƒ
âˆ£
ğ‘¥
)
=
ğ‘
(
ğ‘¥
âˆ£
ğœƒ
)
ğ‘
(
ğœƒ
)
ğ‘
(
ğ‘¥
)
p(Î¸âˆ£x)=
p(x)
p(xâˆ£Î¸)p(Î¸)
	â€‹</p>

<p>Itâ€™s a global operation â€” it recomputes the entire distribution at once. You donâ€™t move continuously through parameter space; you replace the old belief with a new one. This is an instantaneous jump in your belief</p>

<p>Bayesian updating says:</p>

<p>log
â¡
ğ‘
(
ğœƒ
âˆ£
ğ‘¥
)
=
log
â¡
ğ‘
(
ğœƒ
)
+
log
â¡
ğ‘
(
ğ‘¥
âˆ£
ğœƒ
)
âˆ’
log
â¡
ğ‘
(
ğ‘¥
)
logp(Î¸âˆ£x)=logp(Î¸)+logp(xâˆ£Î¸)âˆ’logp(x)</p>

<p>Taking the gradient with respect to 
ğœƒ
Î¸:</p>

<p>âˆ‡
ğœƒ
log
â¡
ğ‘
(
ğœƒ
âˆ£
ğ‘¥
)
=
âˆ‡
ğœƒ
log
â¡
ğ‘
(
ğœƒ
)
+
âˆ‡
ğœƒ
log
â¡
ğ‘
(
ğ‘¥
âˆ£
ğœƒ
)
âˆ‡
Î¸
	â€‹</p>

<p>logp(Î¸âˆ£x)=âˆ‡
Î¸
	â€‹</p>

<p>logp(Î¸)+âˆ‡
Î¸
	â€‹</p>

<p>logp(xâˆ£Î¸)</p>

<p>Each observation has a surprise:</p>

<p>surprise
(
ğ‘¥
)
=
âˆ’
log
â¡
ğ‘
ğœƒ
(
ğ‘¥
)
surprise(x)=âˆ’logp
Î¸
	â€‹</p>

<p>(x)</p>

<p>The gradient of this surprise with respect to 
ğœƒ
Î¸:</p>

<p>âˆ‡
ğœƒ
(
âˆ’
log
â¡
ğ‘
ğœƒ
(
ğ‘¥
)
)
=
âˆ’
âˆ‡
ğœƒ
log
â¡
ğ‘
ğœƒ
(
ğ‘¥
)
âˆ‡
Î¸
	â€‹</p>

<p>(âˆ’logp
Î¸
	â€‹</p>

<p>(x))=âˆ’âˆ‡
Î¸
	â€‹</p>

<p>logp
Î¸
	â€‹</p>

<p>(x)</p>

<p>tells you how to adjust your model to make the observed event less surprising next time.</p>

<p>For any differentiable probability distribution $p_\theta(x)$:</p>

\[\nabla_\theta p_\theta(x) = p_\theta(x)\ \nabla_\theta \log p_\theta(x)\]

<p>gradient descent on log-likelihood arises as an approximation of Bayesâ€™ rule when you linearize the posterior around its maximum</p>

<p>So the chain is:</p>

<p>Logic (absolute truths), Probability (graded plausibility), Information (change in plausibility after surprise).</p>

<p>Gradients of information (log-probability) with respect to parameters  (score functions) show how beliefs should change.</p>

<p>Entropy: Shannon averaged surprise across the whole distribution, defining entropy as expected information. Thatâ€™s the measure of uncertainty before observation.</p>

<p><strong>Belief Revision as Gradient Descent</strong></p>

<p>Every form of empirical learning â€” from a human forming intuitions to a neural network tuning weights â€” can be viewed as gradient descent on surprise.</p>

<p>The beliefs are encoded in parameters Î¸\thetaÎ¸.. The data are observations from the world.. The loss is information (âˆ’log P). The gradient tells us how beliefs should move to make the world less surprising.</p>

<p>Logic tells us what must follow. Probability tells us what is likely. Information tells us how surprised we should be. The gradient of information tells us how to change our beliefs in light of that surprise.</p>

<p>This is the architecture of learning: the world generates events, those events surprise us by amounts determined by our current beliefs, and the gradient tells us exactly how to revise those beliefs to be less surprised next time.</p>

<p>Classical Intelligence is prowess at symbol manipulation.  Anchored in the real world, learning is the organized reduction of surprise, and the gradient of information is the compass that points the way.v</p>

<hr />

<p>This is the cross-entropy loss â€” the standard loss for almost all modern neural networks (softmax classifiers, transformers, etc.). It measures the average surprise of the model about true outcomes.</p>

<p>Score function: âˆ‡Î¸â€‹logÏ€Î¸â€‹(aâˆ£s). How surprised the model was to take action aaa. And in what direction in parameter space we should move to make that action less (or more) surprising.</p>

<p>When we are reasoning about uncertain events in the real world, log likelihood of an event measures â€œsurpriseâ€.</p>

<p>Reinforce:</p>

<p>If a surprising action led to high reward, reduce its surprise next time - make it more expected. Shift probability mass toward actions that surprised us but worked well.</p>

<p>Learning from experience is about reducing surprise about what works. REINFORCE works by adjusting the policy to make rewarding surprises less surprising â€” and the log-likelihood gradient is exactly the mathematical expression of that process.</p>

<hr />

<p>Classical and predicate logic reflect a linguistic fixation on truth. Classical and predicate logic emerge from the linguistic turn, which made philosophy focus on language rather than worldly inference.</p>

<p>Machine learning and information theory reorient us toward processes of inference and information flow â€” a logic of knowing, not merely saying. reasoning in real systems (humans, organisms, AIs) is about updating under uncertainty, not maintaining consistency under fixed truth. Information is not about truth but about constraint and possibility â€” how systems reduce uncertainty through interaction</p>

<p>The linguistic turn (Frege â†’ Russell â†’ Wittgenstein â†’ Carnap â†’ early analytic philosophy) was motivated by the belief that philosophyâ€™s task is to clarify the logical form of language, not to describe reality directly.
Logic became a theory of meaning and reference, not of causation or information. Classical and first-order (predicate) logic are systems of syntactic entailment.
They formalize when a statement 
ğ‘„
Q follows necessarily from a set of statements 
ğ‘ƒ
P under fixed interpretation rules.</p>

<p>So, whereas Russell and Frege made logic a mirror of language, Whitehead made logic a mirror of evolution and interaction</p>

<p>He argued that the world isnâ€™t composed of static substances, but processes â€” events that â€œprehendâ€ or take account of one another.</p>

<p>Logic, therefore, should not model relations among propositions, but relations among actualities in becoming.</p>

<p>â€œNature is a structure of evolving processes. The reality is the process.â€</p>

<p>He anticipated the idea that information is not a property of sentences, but of relations among events</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:cox" role="doc-endnote">
      <p>Cox asked: What if we generalize this to continuous degrees of belief, while preserving logical consistency (e.g., consistency with conjunction and negation rules)? He derived that any system obeying these logical consistency constraints must be isomorphic to probability theory â€” i.e., plausibility behaves like probability.Â <a href="#fnref:cox" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:bits" role="doc-endnote">
      <p>This is equivalent to: how many bits (or yes/no questions) would suffice to convey a piece of information. Coin flip surprise $= 1$ bit. Die roll surprise $= \log_2(6) \approx 2.58$ bits Together: coin + die outcome surprise should be $1 + 2.58 \approx 3.58$ bits. Imagine a telegraph operator sending symbols (dots/dashes, or letters). Some symbols occur more often than others (â€œEâ€ more than â€œZâ€). To send messages efficiently, weâ€™d like frequent symbols to use shorter codes, rare symbols to use longer codes (like Morse code).Â <a href="#fnref:bits" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:addition" role="doc-endnote">
      <p>It makes information quantifiable We can measure â€œbits of surpriseâ€ without touching meaning or context. That universality is why Shannonâ€™s theory underlies everything from file compression to genetics. From Multiplication to Addition. When you want to combine many uncertainties (as in predicting outcomes across multiple steps), multiplying tiny probabilities quickly becomes intractable. Humans and our tools reason better in linear terms: sums, averages, margins. Nature speaks multiplication. We translate it into addition so that prediction and control become tractable.. Whenever you have probabilities that multiply (e.g., joint or conditional probabilities), their logarithm turns products into sums: This is numerically stable and easier to differentiate, especially since probabilities are often tiny (so log keeps them in a manageable numerical range). We almost never optimize raw probabilities: we optimize log probabilities. But is this just a neat mathematical trick to make computation practical? Or is there something more to log probability? Is there an underlying intuition that ties its appearance evewhere?Â <a href="#fnref:addition" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET