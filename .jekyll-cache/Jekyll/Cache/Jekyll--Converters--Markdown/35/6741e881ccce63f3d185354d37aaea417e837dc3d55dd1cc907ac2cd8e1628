I"«"<p>Tips: For an algorithm that just needs min or max value to work at eachs step, and node the whole sorted order, one can reduce time complexity from O(nlogn) to (O(n)) by working with the kth smallest (or largest) element instead of min or max. Here k should be a linear function of n. For example, k can be the median (k = n/2). Median can be found in O(n) times</p>

<p>It is frequently the case that preprocessing the data or using a priority queue, one can make the greedy choice quickly (O(1)), making the algorithm efficient.</p>

<hr />

<blockquote>
  <p>Given a set of activities A with start and end times, find a maximum subset of mutually compatible activities.</p>
</blockquote>

<hr />

<p><strong>First Approach</strong> <em>Iterative, Exhaustive Search</em></p>

<ul>
  <li>Iterate over each possible subset of activities</li>
  <li>Checking for comptability</li>
  <li>Out of all subsets which are compatble, take the maximum length subset</li>
</ul>

<p>Number of problems solved: \( 2^n \)       <br />
Time to solve each subproblem : \( O(n) \)</p>

<hr />

<p><strong>Second approach</strong> <em>Top-down, decrease-by-one recursion</em></p>

<p>Strategy: Solve an easier problem (aka subproblem).    <br />
Why? Maybe the problem has an optimal substructure: an optimal solution cae constructed from optimal solutions to its subproblems. 
Which easier problem should we try? Let‚Äôs try decrease-by-one and conquer.
Given an optimal substucture, if we knew which \(a_k \) to choose, we could construct an optimal solution by selecting  \(a_k \) alongwitht he all actvities in optimal soluton to subproblems.</p>

<ul>
  <li>Assume a particular activity \(a_k \) is part of the optimal solution</li>
  <li>Solve the subproblem: select a maximal subset of compatible activies from the activities which are compatible with \(a_k \).</li>
  <li>Loop over all possible activities \(a_k \) and choose maximum.</li>
</ul>

<p>Let \( A = [a_0, a_1‚Ä¶..a_k‚Ä¶a_{n-1}] \) be the set of all activities.</p>

<p>Let  \(A_k \) denote the set of all activities which are compatible with \(a_k \). This set will be smaller that \(A \), since it doesn‚Äôt contain \(a_k \). Then,</p>

<p>\[  Opt(\it{A}) = \max_{k=0}^{n-1} Opt(\it{A_k} ) + 1  \]</p>

<p>Recursion Tree:</p>

<p>Number of subproblems: \( 2^n \)</p>

<p>Number of function calls:</p>

<p>\[ T(n) = n + nT(n-1) \]</p>

<p>\[ 1 + n + n(n-1) + n(n-1)(n-2)‚Ä¶+ n! \] 
 \[ = n!(1 + 1/2! + 1/3! + ‚Ä¶. + 1/n!) =   \Theta (en!) \]</p>

<p>Clearly, this is even worse than the brute force approach!</p>

<hr />

<p><strong>Third Approach</strong> <em>Top-down, divide-and-conquer recursion</em></p>

<p>For the easier subprobelm to solve, let‚Äôs try better than the decrease-by-one and conquer strategy: divide-and-conquer.</p>

<p>Here we partition the problem into non-overlapping problems. If the optimal solution to the overall problem can be calculated from the optimal solution to these problems, we‚Äôd have optimal substructure.</p>

<ul>
  <li>Assume a particular activity \(a_k \) is part of optimal solution</li>
  <li>Define the easier problems:
    <ul>
      <li>The set activities that finish before \(a_k \) starts: \(Before(a_k) \)</li>
      <li>The set of activities that start after \(a_k \) finishes: \(After(a_k) \)</li>
    </ul>
  </li>
  <li>Solve the easier problems. If the optimal solution contains \(a_k \), then the optimal solution should contain the optimal solutions to \(Before(a_k) \) and \(After(a_k) \)</li>
  <li>Loop over all possible \(a_k \) and choose maximum.</li>
</ul>

<p>\[  Opt(\it{A}) = \max_{k=0}^{n-1} Opt(Before(a_k)) + Opt(After(a_k)) +  1  \]</p>

<p>Recursion Tree:</p>

<p>Number of subproblems:  \(n^2\)</p>

<p>Number of function calls (worst case):</p>

<p>\[ T(n) = \sum_{k=0}^{k=n-1} T(k) + T(n-k) \]</p>

<hr />

<p>Note</p>

<p>\( {A_k} = Before(a_k) \cup After(a_k))    \)</p>

<p>Without memoization, the two approaches do the same computations at each step. However, if we memoize, then the two approaches are wildly different. The first is still exponential, while the latter is polynomial time.</p>

<p>Why?</p>

<p>Consider \(  = B \cup C  \), where  \(B \) and \( C \) correspond to two dijoint intervals. As \(B \) and \( C \) vary over  \(m \) and \( n \) values, they make up \( mn \) unique values of \(  B \cup C  \), whose solutions we need to store. If we  use \(B \) and \( C \) directly instead of \(  B \cup C  \), then we need only store \( m + n \) values.</p>

<hr />

<p><strong>Fourth Approach</strong> <em>Iterative, Bottom Up</em></p>

<p>Note the recursion.</p>

<p>\[  Opt(\it{A}) = \max_{k=0}^{n-1} Opt(Before(a_k)) + Opt(After(a_k)) +  1  \]</p>

<p>Rotate the recursion tree by 90 degrees and see the dependency graph. Observation: The solution to higher cases depend only on solution to base cases. If we solve the bases cases and iterate in the right order, we can solve the problem.</p>

<p>Represent it by a table or a Map.  Let \(c(i,j) \) denote the solution for the set of activities that lie between activity \( i \)  and \( j \), \(S(i,j)\).</p>

<p>If  \(S(i,j) = \Phi \), then \(c[i,j] = 0\) (base case)
Otherwise, let \(k \) be the index of the activity which lies between  activity \( i \)  and \( j \).</p>

<p>From the recursion,</p>

<p>\[  c(i,j) = \max_{k=i+1}^{j-1} (c(i,k) + c(k,j)) +  1  \]</p>

<p>Not all indices \( k \)  between \( i \)  and \( j \)  correspond to such activities: some of them might start before \( i \)  finish, or finish after \( j \)  starts, but we can be certain that hose which do satisfy the constraint are a subset of those whosee indices lie. So all we need to do is loop over these values and check.</p>

<p>We can represent this in the DP table.</p>

<p>Running Time: O(n^3)</p>

<p>Note: We can easily motify this approach to solve the generalization ‚Äúmax total value of activities selected where each activity has a value‚Äù. This special case has each activity valued equally (or equal to 1.)</p>

<hr />

<p><strong>Fifth Approach</strong> <em>A combinatorial insight based on trying various greedy choices</em></p>

<p>Observation: There exist an optimal solution that will have an activity with the earliest finishing time. So this simplifies our choice: Instead of assuming the choice, seeing which subproblems result, finding ther opt, and then making the choice by comparing their results, we can make the choice first.</p>

<p>The optimal substructure varies in how many subproblems we have and how many choices we have in determining which subproblems which subprobems to use. With the insight that greedy choice would work, we could just make that choice! This reducces our subproblems to use from 2 to just 1, and the choice from j-i-1  to just 1 as well.</p>

<p>In the dp approach, we needed to solve the subproblems to help us make the choice. So we proceeded in a bottom up manner. But once we get the greedy insight, we need not solve the subproblem before making the choice.</p>

<p>Intuition: Choosing this activity leavus us the maximum room to bucket other activities in, with the least disturbance to other activities. 
Not that this intuition isn‚Äôt enough: you still need to prove it. For example: choosing the activity with least duration doesn‚Äôt give us the right solution (counter example), and choosing the activity with least disurbance doesn‚Äôt either.</p>

<p>Not that <em>any</em> optimal solutions is not guaranteed to contain this activity: which means it is useless to search for an intuition for why ‚Äúit is true that optimal solution contains the activity with ear*lies finishing time‚Äù, because that statement is false.</p>

<p>Running Time: O(n)</p>

<hr />

<p><strong>Sixth Approach</strong> <em>Inspired by LIS</em></p>

<p>Which easier problem? The maximum number of compatible activities from start to a particular activity which includes the particular activity.
Once we know the solutions to these, we can choose the maximum over all such subproblems. Running Time: O(n^2)</p>

<hr />

<p>0 - 1 Knapsack</p>

<hr />

<h1 id="coin-change">Coin Change</h1>

<p>Strategy: Solve an easier problem (aka subproblem).    <br />
Why? Maybe the problem has an optimal substructure: an optimal solution cae constructed from optimal solutions to its subproblems.</p>

<p><strong>Which easier problem should we try?</strong></p>

<p>There can be multiple ways to answer this, and sometimes more than one approaches might be correct.</p>

<p>For example, in Coin change, we can frame our choice or decision as either of these:</p>

<ul>
  <li>For a particular coin, choose whether this is part of optimal or not. This gives us two choices and two subproblems</li>
  <li>Which coin should we choose as the last one? We have n choices, and n subproblems.</li>
  <li>How many of the first coin should we have?</li>
</ul>

<hr />

<h1 id="lis">LIS</h1>

:ET