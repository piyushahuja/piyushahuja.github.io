I"¸]<blockquote>
  <p>Learning from experience is about reducing surprise about what works.</p>
</blockquote>

<hr />

<p>What would reasoning look like if we model the world as a dynamic entity where many possible worlds are possible, where events unfold, and as we observe what happens alternative realities are destroyed?  What laws of thought would govern reasoning about uncertainty and information?</p>

<p>Logic is absolute. The real world is messy. When faced with uncertainty, instead of abandoning logic, we extend it through probability.  Probability is the study of correct reasoning under uncertainty.</p>

<p>If logic deals in truth values: 0 or 1, probability deals in degrees of plausibility: [0,1]. Instead of propositions, we study events and evidence.</p>

<p>So:</p>

<p>P(A) = 1 =&gt; I know A is true.</p>

<p>P(A) = 0 =&gt; I know A is false.</p>

<p>Intermediate values =&gt; ‚ÄúI don‚Äôt know; A is more or less plausible.‚Äù</p>

<p>Probabilities are thus generalized truth values.<sup id="fnref:cox" role="doc-noteref"><a href="#fn:cox" class="footnote" rel="footnote">1</a></sup></p>

<p>In logic, inference is deductive: If A implies B, and A is true, then B must be true.</p>

<p>In probability, Bayesian inference is inductive: If A supports B with likelihood, and A is plausible, then B‚Äôs plausibility updates by Bayes‚Äô rule.</p>

<p>A probabilistic model $p_\theta(x)$ encodes your beliefs about how likely different observations $x$ are, given parameters $\theta$. When you observe a new data point $x$, you want to update those beliefs. The rule for inference here is called Bayes‚Äô rule:</p>

\[p(\theta \mid x) \propto p(x \mid \theta)\,p(\theta)\]

<p>Bayesian inference would treat  ùúÉ as uncertain and update a whole posterior.</p>

<hr />

<p>There are certain dynamics that play out when we model the world as events with uncertainty and reason about it:</p>

<p><strong>For independent events, likelihoods are multiplicative.</strong> If one event is risky, and the other is risky, the risk of both occuring is muliplicative. When evidence accumulates, it‚Äôs through conjunction: Data point 1 and Data point 2 and Data point 3. The likelihood of all those observations is the product of their individual likelihoods (under independence).</p>

<p>Consider several examples of multiplicative probability in independent events:</p>

<ul>
  <li>
    <p><strong>Flipping 10 coins:</strong><br />
The probability of obtaining one specific sequence of heads and tails is<br />
\(\left(\frac{1}{2}\right)^{10}\) since each coin flip is independent with probability $\frac{1}{2}$ for each outcome.</p>
  </li>
  <li>
    <p><strong>Observing 100 i.i.d. data points:</strong><br />
The likelihood of observing a particular sequence is the product of the individual probabilities:
\(p(x_1, x_2, \dots, x_{100}) = \prod_{i=1}^{100} p(x_i)\)
provided the $x_i$ are independent and identically distributed.</p>
  </li>
  <li>
    <p><strong>Joint risk of independent failures:</strong><br />
Suppose a machine has a $\frac{1}{10}$ chance of failure each year.<br />
If you run two machines independently in parallel, the probability that both fail is:
\(\frac{1}{10} \times \frac{1}{10} = \frac{1}{100}\)</p>
  </li>
  <li>
    <p><strong>False positives in independent tests:</strong><br />
If the chance a suspect is guilty purely by random chance is $\frac{1}{100}$,<br />
and each of two independent tests has a $\frac{1}{10}$ false-positive rate,<br />
then the chance both tests indicate ‚Äúguilty‚Äù by random chance is:
\(\frac{1}{10} \times \frac{1}{10} = \frac{1}{100}\)</p>
  </li>
</ul>

<!-- Joint occurrence of facts is modelled by intersection. It is the natural operation for ‚Äúcompounding reality‚Äù.   




The uncertainty of world-states is what ties possible worlds to probability.  

Possible Worlds: Each independent degree of freedom multiplies the number of possible worlds. With nnn binary independent factors, you don‚Äôt get n+1n+1n+1 outcomes ‚Äî you get 2n2^n2n. What compounds is the size of the state space of reality. 

Each new fact or discovered outcome slices away possible worlds. Learning both A and B means we‚Äôre narrowing further, i.e. moving into a smaller intersection.  -->

<p>When you want to combine many uncertainties (as in predicting outcomes across multiple steps), multiplying tiny probabilities quickly becomes intractable.  Humans and our tools reason better in linear terms: sums, averages, margins.
Converting probability to their logarithm turns products into sums: this is numerically stable and easier to differentiate,</p>

<hr />

<p>Once an uncertain event in the world is resolved, we get some information.</p>

<p><strong>How much do we measure information? How should information from an observation behave when we reason about uncertainty?</strong></p>

<p>Let‚Äôs look at some events:</p>

<p>‚ÄúThe sun rose this morning.‚Äù If we assume the laws of nature hold and the sun rises everyday, this observation hardly gives us any information.</p>

<p>‚ÄúThe stock market crashed today.‚Äù This is a rare event, and it gives us a lot of information.</p>

<p>If something that rare events give lots of information. Common events give little information. Impossible events give infinite information - you‚Äôd have to revise your whole worldview. Thus, the rarity of an event must set its information content.</p>

<p>Before Claude Shannon, people used to think of information in terms of semantics, i.e. a message‚Äôs meaning.  But Shannon‚Äôs  insight was to anchor information to how surprising an event is i.e. ‚Äúhow unlikely is the world we observe, given our expectations?‚Äù</p>

<p>Shannon reasoned that any measure of information must have these two properties:</p>
<ol>
  <li>It should be a monotonic decreasing function of probability.</li>
  <li>Information should add up for independent events.</li>
</ol>

<p>To see this. suppose you want to know the outcome of two independent experiments:</p>

<ul>
  <li>Flip a fair coin $\left(P = \frac{1}{2}\right)$</li>
  <li>Roll a fair die $\left(P = \frac{1}{6}\right)$</li>
</ul>

<p>To identify both outcomes, you‚Äôd need the same number of yes/no questions as asking them separately, then adding the answers.</p>

<p>It can be proven function that satisfies both properties is the logarithm:</p>

\[I(x) = -\log P(x)\]

<p>$-\log P(x)$ is the unique measure of information.<sup id="fnref:bits" role="doc-noteref"><a href="#fn:bits" class="footnote" rel="footnote">2</a></sup><sup id="fnref:addition" role="doc-noteref"><a href="#fn:addition" class="footnote" rel="footnote">3</a></sup></p>

<!-- 
It ties prediction and control together Prediction: information tells you how much uncertainty was reduced by seeing the actual outcome. Control: information tells you how much ‚Äúroom to maneuver‚Äù you gain when you know what actually happened.

Imagine a telegraph operator sending symbols (dots/dashes, or letters). Some symbols occur more often than others (‚ÄúE‚Äù more than ‚ÄúZ‚Äù). To send messages efficiently, we‚Äôd like frequent symbols to use shorter codes, rare symbols to use longer codes (like Morse code).


It‚Äôs operational If you want to transmit outcomes over a noisy channel, you need enough symbols to cover the worst-case surprise. That leads directly to coding theorems and compression limits. -->
<!-- 
Logic says: ‚ÄúIf X is true, then Y must follow.‚Äù Probability says: ‚ÄúGiven X, Y becomes more plausible.‚Äù Information says: ‚ÄúHow much more plausible did Y get after we actually observed X?‚Äù -->

<hr />
<p><strong>How would we learn from information?</strong></p>

<p>Say a probabilistic model $p_\theta(x)$ encodes your beliefs about how likely different observations $x$ are, given parameters $\theta$.</p>

<p>Each observation has a surprise:</p>

\[\text{surprise}(x) = -\log p_\theta(x)\]

<p>The gradient of this surprise with respect to $\theta$:</p>

\[\nabla_\theta \left( -\log p_\theta(x) \right ) = -\nabla_\theta \log p_\theta(x)\]

<p>tells you how to adjust your model to make the observed event less surprising next time.</p>

<p>$s_\theta(x) = \nabla_\theta \log p_\theta(x)$ (‚Äúscore function‚Äù) is a vector in parameter space. It tells you:</p>

<p>‚ÄúIf the world looked like $x$, how should my beliefs about $\theta$ move?‚Äù</p>

<p>To update a probabilistic model from data, we must change parameters in the direction that reduces the surprise of observed outcomes.</p>

<p>We can think of it as the infinitesimal form of Bayesian reasoning.  Recall that Bayes‚Äô rule of inference tells you how to update a full probability distribution after seeing data:</p>

\[p(\theta \mid x) = \frac{p(x \mid \theta)\; p(\theta)}{p(x)}\]

<p>This is a global operation: it recomputes the entire distribution at once. You don‚Äôt move continuously through parameter space; you replace the old belief with a new one. This is an instantaneous jump in your belief.</p>

<p>Bayesian updating says:</p>

\[\log p(\theta \mid x) = \log p(\theta) + \log p(x \mid \theta) - \log p(x)\]

<p>Taking the gradient with respect to $\theta$:</p>

\[\nabla_\theta \log p(\theta \mid x) = \nabla_\theta \log p(\theta) + \nabla_\theta \log p(x \mid \theta)\]

<p>That‚Äôs the Bayesian gradient update:</p>
<ul>
  <li>A likelihood term that pulls toward data ($\nabla_\theta \log p(x \mid \theta)$).  This is the same as the score function.</li>
  <li>A prior term that regularizes beliefs ($\nabla_\theta \log p(\theta)$)</li>
</ul>

<p>Here priors express inductive biases explicitly. the posterior update direction (how beliefs about 
ùúÉ should move) is given by the score function plus the gradient of the prior.</p>

<p>In deep learning, priors are implicit: L2 weight decay Penalizes large weights.	Imposes a Gaussian prior on weights.
Dropout	Randomly removes neurons to prevent overfitting.	Approximate model averaging (variational posterior).
Early stopping	Prevents overfitting by halting optimization.	Implicitly imposes a prior favoring smoother solutions.</p>

<p>Regularization via priors: P Small data robustness: Bayesian models can ‚Äúbake in‚Äù prior knowledge to perform well even with limited data</p>

<p>Bayes‚Äô rule updates beliefs after seeing data (a ‚Äúglobal‚Äù jump). The score function is the local direction of that update ‚Äî how posterior belief changes as a function of parameters. 
In Modern ML,Parameters are treated as deterministic (learned weights)., not a random variable with a prior distribution. gradient descent on log-likelihood arises as an approximation of Bayes‚Äô rule when you linearize the posterior around its maximum
Bayes‚Äô rule:
‚Üí ‚ÄúUpdate your beliefs after observing the data.‚Äù</p>

<p>Score function gradient:
‚Üí ‚ÄúContinuously nudge your beliefs toward making the observed data less surprising.</p>

<hr />

<p><strong>The Information Geometry</strong></p>

<p><strong>Distributions as Points on a Manifold</strong></p>

<p>Imagine every possible probability distribution $p_\theta(x)$ as a point on a smooth surface ‚Äî a statistical manifold.<br />
Here, $\theta = (\theta_1, \theta_2, \ldots, \theta_n)$ are the parameters (like mean and variance for a Gaussian). Changing $\theta$ moves you to a nearby distribution.</p>

<p>So, instead of Euclidean space with coordinates $x, y, z$, you have a space of probability models with coordinates $\theta_1, \theta_2, \ldots, \theta_n$.</p>

<p>The gradient of $\log p_\theta(x)$ gives the direction of steepest information gain.</p>

<p>So, for the <em>score function</em>
\(u_\theta(x) = \nabla_\theta \log p_\theta(x)\)
this tells you how fast the log-likelihood changes per unit change in parameter.</p>

<ul>
  <li>If $u_\theta(x)$ is large, a small change in $\theta$ dramatically alters the likelihood of observing $x$.</li>
  <li>If $u_\theta(x)$ is small, $\theta$ hardly affects the likelihood of that observation.</li>
</ul>

<p>That‚Äôs sensitivity: the rate at which information (log-probability) changes when parameters move.</p>

<p>Interpret $\log p_\theta(x)$ as the information content (the negative surprise) of observing $x$.<br />
Then $u_\theta(x)$ tells you how that information changes as you tweak $\theta$.</p>

<ul>
  <li>If $u_\theta(x) = 0$, then $\theta$ has no effect on the information content of $x$.</li>
  <li>If $u_\theta(x) \neq 0$, then $x$ becomes more or less ‚Äúsurprising‚Äù as $\theta$ changes.</li>
</ul>

<p>So, the score function measures how sensitive your informational beliefs are to parameter changes ‚Äî literally, how ‚Äúinformation‚Äù reacts to $\theta$.</p>

<p>For any differentiable probability distribution $p_\theta(x)$:
\(\nabla_\theta p_\theta(x) = p_\theta(x)\ \nabla_\theta \log p_\theta(x)\)</p>

<p>The change in probability of an outcome under a parameter change is proportional to its current probability times its informational sensitivity. This shows how information flows in a probability manifold: we don‚Äôt just care about the change in raw values, but change weighted by likelihood.</p>

<table>
  <tbody>
    <tr>
      <td>Information geometry sees $\pi_\theta(a</td>
      <td>s)$ as a point on a manifold of probability distributions.</td>
    </tr>
  </tbody>
</table>

<p>The gradient
\(\nabla_\theta \log \pi_\theta(a|s)\)
is then a tangent vector in that space ‚Äî it tells you in which direction (in distribution space) the probability mass shifts when you move the parameters.</p>

<p>In other words:
\(\nabla_\theta \pi_\theta(a|s) = \pi_\theta(a|s) \cdot \left( \text{tangent direction of change} \right)\)</p>

<p>So,
\(\nabla_\theta \log \pi_\theta(a|s)\)
isn‚Äôt just a computational trick ‚Äî it‚Äôs the natural coordinate of the tangent space of the statistical manifold.
It‚Äôs how you represent ‚Äúinfinitesimal motion‚Äù in the geometry of probabilities.</p>

<p>The Hessian of $\log p_\theta(x)$ defines the curvature: i.e., how fast uncertainty changes as you move in parameter space. That curvature is the geometry of information.</p>

<p>When you compute geodesics (shortest paths) on this manifold, they represent statistically efficient transitions between models ‚Äî the most ‚Äúnatural‚Äù way to change a distribution.</p>

<p>Information geometry gives a bridge between the two paradigms:</p>
<ul>
  <li>The score function is the infinitesimal generator of information flow on the statistical manifold.</li>
  <li>It quantifies how responsive your probabilistic model is ‚Äî how much ‚Äúinformational signal‚Äù parameter changes produce in the likelihood landscape.</li>
</ul>

<p><strong>Frequentist:</strong> focuses on point estimates on a manifold of distributions ($\hat{\theta}$).<br />
<strong>Bayesian:</strong> maintains a distribution over that manifold ($p(\theta|D)$).</p>

<p>From this perspective:</p>
<ul>
  <li>Deep learning corresponds to following the gradient flow on the information manifold ‚Äî finding one low-loss point.</li>
  <li>Bayesian inference corresponds to computing the entire posterior distribution on that manifold.</li>
</ul>

<hr />

<p><strong>Learning as belief revision to reduce surprise</strong></p>

<!-- 
Entropy: Shannon averaged surprise across the whole distribution, defining entropy as expected information. That‚Äôs the measure of uncertainty before observation. -->

<p>Every form of empirical learning ‚Äî from a human forming intuitions to a neural network tuning weights ‚Äî can be viewed as gradient descent on surprise. Learning from experience is about reducing surprise about what works.</p>

<p>The beliefs are encoded in parameters Œ∏\thetaŒ∏.. The data are observations from the world The loss is information (‚àílog P). The gradient tells us how beliefs should move to make the world less surprising.</p>

<p>So the chain is: Logic (absolute truths), Probability (graded plausibility), Information (change in plausibility after surprise). Gradients of information (log-probability) with respect to parameters  (score functions) show how beliefs should change. Logic tells us what must follow. Probability tells us what is likely. Information tells us how surprised we should be. The gradient of information tells us how to change our beliefs in light of that surprise.</p>

<p>This is the architecture of learning: the world generates events, those events surprise us by amounts determined by our current beliefs, and the gradient tells us exactly how to revise those beliefs to be less surprised next time.</p>

<p>Examples:</p>

<p>This is the cross-entropy loss ‚Äî the standard loss for almost all modern neural networks (softmax classifiers, transformers, etc.). It measures the average surprise of the model about true outcomes.</p>

<p>Reinforce: If a surprising action led to high reward, reduce its surprise next time - make it more expected. Shift probability mass toward actions that surprised us but worked well REINFORCE works by adjusting the policy to make rewarding surprises less surprising ‚Äî and the log-likelihood gradient is exactly the mathematical expression of that process.</p>

<p>$Log Probability$ seems to jump out everwhere in ML, not dissimilar to how $\pi$ comes up again and again in geometry.</p>

<p>nstead of computing posterior distributions, modern ML optimizes point estimates:</p>

<p>ùúÉ
‚àó
=
arg
‚Å°
max
‚Å°
ùúÉ
log
‚Å°
ùëù
(
ùê∑
‚à£
ùúÉ
)
Œ∏
‚àó
=arg
Œ∏
max
	‚Äã</p>

<p>logp(D‚à£Œ∏)</p>

<p>That‚Äôs MLE (or MAP if you add priors as penalties).</p>

<p>This underlies every neural network, transformer, and foundation model.</p>

<p>You often want to differentiate an expectation in reinforcement learning:</p>

\[\nabla_{\theta} \mathbb{E}_{a \sim \pi_{\theta}(\cdot \mid s)}[f(a)]\]

<p>The gradient passes through the distribution $\pi_{\theta}$, so you get:</p>

\[\nabla_{\theta} \mathbb{E}_{\pi_{\theta}} [f(a)] = \mathbb{E}_{\pi_{\theta}} \left[ f(a) \nabla_{\theta} \log \pi_{\theta}(a \mid s) \right]\]

<p>This is the score function trick or log-derivative trick, and it allows you to estimate gradients without differentiating through the random sampling process</p>

<p>In Supervised Learning for classification, we minimize the negative log likelihood of the correct class (cross entropy loss):</p>

\[L = -\log p_\theta(y_{\text{true}} \mid x)\]

<p>In Diffusion Models, the training objective minimizes the KL divergence (difference of log-probabilities) between the forward and reverse processes.</p>

<p>The Kullback-Leibler (KL) divergence is</p>

\[D_{KL}(p \| q) = \mathbb{E}_{p}\left[ \log p(x) - \log q(x) \right]\]

<p>In LLMs:</p>

<p>An LLM (say GPT) is trained to maximize next-token likelihood:</p>

\[\max_\theta \sum_t \log p_\theta(x_t \mid x_{&lt;t})\]

<p>This is maximum likelihood estimation (MLE) over a corpus.
Each term says: ‚Äúadjust Œ∏ so the model‚Äôs predicted distribution over the next token matches what actually occurred, given the past.‚Äù</p>

<p>So at face value, it‚Äôs a frequentist optimization problem: minimize negative log-likelihood.</p>

<p>From information theory, the training loss is the cross-entropy between the data distribution $p^*(x)$ and the model $p_\theta(x)$:</p>

\[L = -\mathbb{E}_{p^*} \left[ \log p_\theta(x) \right]\]

<p>Minimizing this is equivalent to minimizing the KL divergence:
\(\mathrm{KL}\left(p^* \;\|\; p_\theta\right)\)</p>

<p>That‚Äôs exactly what Bayesian updating does ‚Äî it moves beliefs $p_\theta$ to reduce divergence from the observed evidence.</p>

<hr />

<p><strong>Comparison with the static classical view</strong></p>

<p>Classical Intelligence is prowess at symbol manipulation.  Anchored in the real world, learning is the organized reduction of surprise, and the gradient of information is the compass that points the way.</p>

<p>Classical and predicate logic reflect a linguistic fixation on truth. Classical and predicate logic emerge from the linguistic turn, which made philosophy focus on language rather than worldly inference.</p>

<p>Machine learning and information theory reorient us toward processes of inference and information flow ‚Äî a logic of knowing, not merely saying. reasoning in real systems (humans, organisms, AIs) is about updating under uncertainty, not maintaining consistency under fixed truth. Information is not about truth but about constraint and possibility ‚Äî how systems reduce uncertainty through interaction</p>

<p>The linguistic turn (Frege ‚Üí Russell ‚Üí Wittgenstein ‚Üí Carnap ‚Üí early analytic philosophy) was motivated by the belief that philosophy‚Äôs task is to clarify the logical form of language, not to describe reality directly. Logic became a theory of meaning and reference, not of causation or information. Classical and first-order (predicate) logic are systems of syntactic entailment.They formalize when a statement Q follows necessarily from a set of statements  P under fixed interpretation rules.</p>

<p>So, whereas Russell and Frege made logic a mirror of language, Whitehead made logic a mirror of evolution and interaction. He argued that the world isn‚Äôt composed of static substances, but processes ‚Äî events that ‚Äúprehend‚Äù or take account of one another. Logic, therefore, should not model relations among propositions, but relations among actualities in becoming.</p>

<p>‚ÄúNature is a structure of evolving processes. The reality is the process.‚Äù</p>

<p>He anticipated the idea that information is not a property of sentences, but of relations among events</p>

<p>Let‚Äôs lay it out from first principles to modern ML:</p>

<p>Uncertainty ‚Üí there are many possible worlds.</p>

<p>Information ‚Üí reduction of uncertainty when one world is revealed.</p>

<p>Surprise ‚Üí 
‚àí
log
‚Å°
ùëù
(
ùë•
)
‚àílogp(x), the measure of how unexpected that world was.</p>

<p>Entropy ‚Üí expected surprise; baseline uncertainty.</p>

<p>Learning ‚Üí minimize expected surprise.</p>

<p>MLE ‚Üí find parameters that make observed data least surprising.</p>

<p>Regression/classification losses ‚Üí special cases of MLE under different likelihoods.</p>

<p>Cross-entropy ‚Üí information-theoretic name for expected surprise.</p>

<p>Score function gradients ‚Üí local rule for how to change beliefs.</p>

<p>Bayes‚Äô rule ‚Üí global version of that same update.</p>

<p>Control ‚Üí information buys freedom; less uncertainty = more precise, adaptive action.</p>

<p>Reinforcement learning ‚Üí learn to reduce surprise about rewarding outcomes.</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:cox" role="doc-endnote">
      <p>Cox asked: What if we generalize this to continuous degrees of belief, while preserving logical consistency (e.g., consistency with conjunction and negation rules)? He derived that any system obeying these logical consistency constraints must be isomorphic to probability theory ‚Äî i.e., plausibility behaves like probability.¬†<a href="#fnref:cox" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:bits" role="doc-endnote">
      <p>This is equivalent to: how many bits (or yes/no questions) would suffice to convey a piece of information. Coin flip surprise $= 1$ bit. Die roll surprise $= \log_2(6) \approx 2.58$ bits Together: coin + die outcome surprise should be $1 + 2.58 \approx 3.58$ bits.¬†<a href="#fnref:bits" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:addition" role="doc-endnote">
      <p>It makes information quantifiable. We can measure ‚Äúbits of surprise‚Äù without touching meaning or context. That universality is why Shannon‚Äôs theory underlies everything from file compression to genetics.  When you want to combine many uncertainties (as in predicting outcomes across multiple steps), multiplying tiny probabilities quickly becomes intractable. Humans and our tools reason better in linear terms: sums, averages, margins. We translate it into addition so that prediction and control become tractable.¬†<a href="#fnref:addition" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET