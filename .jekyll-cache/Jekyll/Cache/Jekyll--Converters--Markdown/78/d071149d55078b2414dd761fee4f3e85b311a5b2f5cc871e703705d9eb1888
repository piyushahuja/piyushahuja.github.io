I"©]<h1 id="what-is-docker">What is Docker?</h1>

<p>Docker is a form ofÂ OS-level virtualisationÂ whoseÂ purposeÂ is, to quote:</p>
<blockquote>
  <p>Package up code and all its dependencies so the application runs quickly and reliably from one computing environment to another</p>
</blockquote>

<hr />

<h1 id="why-docker">Why Docker?</h1>

<ul>
  <li>
    <p><strong>Speed</strong>: isolated, consistent and repeatable environments
for your developers to fire up a new version of the application that a collogue may have written requires muddling around in Git and using branches â€“ this interrupts work flows and can lead to lose of work if â€œgit stashâ€ isnâ€™t used right. With a container: they just run it.</p>
  </li>
  <li>
    <p><strong>Portability</strong>: Docker images define abstract and immutable run time environment independent of the underlying hardware platform and packages it into a well defined and efficient image specification. With Dockerhub, we have a global distribution network
First up: portability and allowing anyone, on any hardware, and any OS to run the application with a single command. All they need to do is install Docker and â€œdocker runâ€ will do the rest. With a TAR file based deployment, they need TAR, they need to know how-to deploy the application and manage the files; they may need some runtime on their system like Ruby, Python or NodeJS; and they cannot restrict the resources the application is usingâ€¦</p>
  </li>
  <li>
    <p><strong>Simplifies your infrastructure requirements</strong>
you can restrict the resources the application is using inside of a container much easier than you can with one that is not.</p>
  </li>
  <li>
    <p><strong>Automation</strong> is first class citizen: Dockerfile, DockerCompose
one final point here: Docker Compose. Youâ€™re missing out on a lot of free stuff here. Itâ€™s so easy to write a Docker Compose file that brings up everything needed by the application - databases, caches, mocked APIs - allowing anyone in your organisation to bring up the application on their laptop and play with it, including the CEO, DBAs, QA, the receptionist.â€¨</p>
  </li>
</ul>

<p>Example: You donâ€™t have to do these things (installing build tools, dependencies to build an application) on every new deployment! Instead you can build the applicationone and distribute it as an â€œexecutableâ€ (without the build tools and dependencies)</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Source Openrc-hgi.sh
Install python3
Install pip
Install setuptools wheel 
Pip install -r requirements.txt (openstack, dateutil, json,â€¦.)
Npm install (vue, bootstrapvue, â€¦)
</code></pre></div></div>

<p><a href="://news.ycombinator.com/item?id=18502168">How is Docker better than just packaging the app with tar?</a></p>

<hr />

<h1 id="docker-architecture">Docker architecture</h1>

<p>Docker uses a client-server architecture. The DockerÂ clientÂ talks to the DockerÂ daemon, which does the heavy lifting of building, running, and distributing your Docker containers. The Docker client and daemonÂ canÂ run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. When people say â€œDockerâ€ they typically meanÂ Docker Engine, the client-server application made up of the Docker daemon, a REST API that specifies interfaces for interacting with the daemon, and a command line interface (CLI) client that talks to the daemon (through the REST API wrapper). Docker Engine acceptsÂ dockerÂ commands from the CLI, such asÂ docker run <image>,Â docker psÂ to list running containers,Â docker image lsÂ to list images, and so on.</image></p>

<p>Docker EngineÂ is a client-server application with these major components:</p>
<ul>
  <li>A server which is a type of long-running program called a daemon process (theÂ dockerdÂ command).â€¨</li>
  <li>A REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do.â€¨</li>
  <li>A command line interface (CLI) client (theÂ dockerÂ command)</li>
</ul>

<p>The CLI uses the Docker REST API to control or interact with the Docker daemon through scripting or direct CLI commands. Many other Docker applications use the underlying API and CLI.
The daemon creates and manages DockerÂ objects, such as images, containers, networks, and volumes.
ï¿¼
The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services.</p>

<p>A DockerÂ registryÂ stores docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for images on Docker Hub by default. You can even run your own private registry. If you use Docker Datacenter (DDC), it includes Docker Trusted Registry (DTR).</p>

<p>When you use theÂ docker pullÂ orÂ docker runÂ commands, the required images are pulled from your configured registry. When you use theÂ docker pushÂ command, your image is pushed to your configured registry</p>

<p><strong>The copy-on-write (CoW) strategy</strong>
Copy-on-write is a strategy of sharing and copying files for maximum efficiency. If a file or directory exists in a lower layer within the image, and another layer (including the writable layer) needs read access to it, it just uses the existing file. The first time another layer needs to modify the file (when building the image or running the container), the file is copied into that layer and modified. This minimizes I/O and the size of each of the subsequent layers.Â </p>

<p>When you start a container, a thin writable container layer is added on top of the other layers. Any changes the container makes to the filesystem are stored here. Any files the container does not change do not get copied to this writable layer. This means that the writable layer is as small as possible.Not only does copy-on-write save space, but it also reduces start-up time. When you start a container (or multiple containers from the same image), Docker only needs to create the thin writable container layer.If Docker had to make an entire copy of the underlying image stack each time it started a new container, container start times and disk space used would be significantly increased. This would be similar to the way that virtual machines work, with one or more virtual disks per virtual machine.</p>

<p><a href="https://stackoverflow.com/questions/43802109/output-of-tail-f-at-the-end-of-a-docker-cmd-is-not-showing/43807880#43807880">Bugs due to Copy-on-Write</a>Â </p>

<p>The docker filesystem uses copy-on-write with itâ€™s layered union fs. So when you write to a file thatâ€™s part of the image, it will first make a copy of that file to the container filesystem which is a layer above all the image layers.Â What that means is when you append a line to the /var/log/cron.log, it will get a new inode in the filesystem and the file that theÂ tailÂ command is following at is no longer the one you see when youÂ docker execÂ into the container.</p>

<hr />

<h1 id="cheatsheet">Cheatsheet</h1>

<p><strong>For deployment of code</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker build <span class="nt">-t</span> mercury/openstack_report_backend:prod <span class="nb">.</span>
docker login <span class="nt">--username</span><span class="o">=</span>mercury <span class="nt">--password</span><span class="o">=</span><span class="k">****</span>
docker push  mercury/openstack_report_backend:prod

<span class="c"># You need to tag your image correctly first with yourÂ registryhost:</span>
docker tag <span class="o">[</span>OPTIONS] IMAGE[:TAG] <span class="o">[</span>REGISTRYHOST/][USERNAME/]NAME[:TAG]
<span class="c">#Then docker push using that same tag.</span>
docker push NAME[:TAG]

docker tag 518a41981a6a myRegistry.com/myImage
docker push myRegistry.com/myImage
</code></pre></div></div>

<p><strong>For debugging</strong></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c"># You can see a list of your running containers with the command,Â docker ps, just as you would in Linux.</span>
docker ps

docker ps <span class="nt">-a</span> <span class="nt">-q</span> 
<span class="c"># -qÂ prints just the container ids (without column headers)</span>
<span class="c"># -fÂ allows you to filter your list of printed containers (in this case we are filtering to only show exited containers) </span>
<span class="c"># -vÂ to avoid dangling volumes</span>


journalctl <span class="nt">-u</span> docker.service | <span class="nb">tail</span> <span class="nt">-n</span> 50 

<span class="c"># to get a bash shell in a container </span>
dockerÂ exec <span class="nt">-it</span> &lt;containerÂ name&gt; /bin/bash 


<span class="c"># Example:  to get environmental variables</span>
docker <span class="nb">exec </span>container bash <span class="nt">-c</span> <span class="s1">'echo "$ENV_VAR"'</span>
docker <span class="nb">exec </span>container <span class="nb">printenv </span>VARIABLE

docker inspect &lt;container name&gt;

<span class="c"># You can display the long-form Id for a container with the command:</span>
docker inspect <span class="nt">--format</span> <span class="s1">'{{ .Id }}&gt;'</span> &lt;container name&gt;

<span class="c"># in a swarm </span>
docker service logs â€”details <span class="nt">--tail</span> 10 &lt;service name&gt;
docker service ps <span class="nt">--no-trunc</span> <span class="c"># to find out which node the service is running on</span>
docker service ps my-ngx <span class="c"># returns a task id</span>

docker service inspect <span class="nt">--pretty</span> &lt;SERVICE-ID&gt;
<span class="c"># to get the container id</span>
docker inspect <span class="nt">-f</span> <span class="s2">"{{.Status.ContainerStatus.ContainerID}}"</span> &lt;task_id&gt; 



<span class="c"># exec inside a swarm service</span>

docker <span class="nb">exec</span> <span class="nt">-it</span> <span class="si">$(</span>docker ps <span class="nt">--filter</span> <span class="s2">"name=my-service."</span> <span class="nt">-q</span><span class="si">)</span> <span class="nb">cat</span> /run/secrets/my-service-secret


</code></pre></div></div>

<p><strong>For Cleanup</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># This will remove all stopped containers and should work on all platforms the same way.</span>
docker container prune

<span class="c"># To  clean up all unused containers, networks, images (both dangling and unreferenced), and optionally, volumes, in one command.will delete ALL unused data (i.e., in order: containers stopped, volumes without containers and images with no containers).</span>
docker system pruneÂ 

docker <span class="nb">rm</span> <span class="sb">`</span>docker ps <span class="nt">--no-trunc</span> <span class="nt">-aq</span><span class="sb">`</span>
docker <span class="nb">rm</span> <span class="si">$(</span>docker ps <span class="nt">-a</span> <span class="nt">-q</span> <span class="nt">--no-trunc</span><span class="si">)</span> <span class="c"># to avoid id clashes </span>
docker ps <span class="nt">-a</span> <span class="nt">-q</span> | xargs docker <span class="nb">rm  
</span>docker <span class="nb">rm</span> <span class="nt">-v</span> <span class="si">$(</span>docker ps <span class="nt">-q</span> <span class="nt">-f</span> <span class="nv">status</span><span class="o">=</span>exited<span class="si">)</span>
docker <span class="nb">rm</span> <span class="si">$(</span>docker ps <span class="nt">-v</span> <span class="nt">-q</span> <span class="nt">-f</span>  <span class="nv">status</span><span class="o">=</span>exited<span class="si">)</span>  

</code></pre></div></div>
<hr />

<h1 id="the-language-of-docker">The language of Docker</h1>

<p>Image
Container: a running image
Service: a group of containers representing the same function (e.g. container replicas in a swarm)
Stack: a group of services + networks  + volumes
Node: a networking address: could be a container, a host etc.
Swarm: a group of nodes acting as one
Task
Config 
ï¿¼</p>

<p>Service vs Container vs Task</p>

<p>Services and container are related but both are different things.
A service can be run by one or multiple containers. WithÂ dockerÂ you can handle containers and withÂ docker-composeÂ you can handle services.
For example:
Letâ€™s say that we have thisÂ docker-compose.ymlÂ file:
web:
  image: example/my_web_app:latest
  expose:
    - 80
  links:
    - db</p>

<p>db:
  image: postgres:latest
This compose file defines two services,Â webÂ andÂ db.
When you runÂ docker-compose up, Asuming that the project directory isÂ test1Â then compose will start 2 containers namedÂ myapp_db_1Â andÂ myapp_web_1.
$ docker ps -a
CONTAINER ID   IMAGE        COMMAND          â€¦      NAMES
1c1683e871dc   test1_web    â€œnginx -gâ€       â€¦      test1_web_1
a41360558f96   test1_db     â€œpostgres -dâ€    â€¦      test1_db_1
So, in this point you have 2 services and 1 container for each.
But you could scale the service namedÂ webÂ to use 5 containers.
$ docker-compose scale web=5
Creating and starting 2 â€¦ done
Creating and starting 3 â€¦ done
Creating and starting 4 â€¦ done
Creating and starting 5 â€¦ done
In this point you have 2 services and 6 containers</p>

<p><strong>AnÂ image</strong>Â is an executable package that includes everything needed to run an applicationâ€“the code, a runtime, libraries, environment variables, and configuration files.</p>

<p><strong>AÂ container</strong> is a runtime instance of an imageâ€“what the image becomes in memory when executed (that is, an image with state, or a user process).  A container is launched by running an image.</p>

<p>Docker images are stored as series of read-only layers. When we start a container, Docker takes the read-only image and adds a read-write layer on top.Â  Images are frozen immutable snapshots of live containers. Containers are running (or stopped) instances of some image. <a href="https://stackoverflow.com/questions/21498832/in-docker-whats-the-difference-between-a-container-and-an-image">Image vs Container</a></p>

<p><strong>AÂ TaskÂ definition</strong>Â is a collection of 1 or moreÂ containerÂ configurations. Some Tasks may need only one container, while other Tasks may need 2 or more potentially linked containers running concurrently. The Task definition allows you to specify which Docker image to use, which ports to expose, how much CPU and memory to allot, how to collect logs, and define environment variables.</p>

<p><strong>AÂ Task</strong>Â is created when you run a Task directly, which launches container(s) (defined in the task definition) until they are stopped or exit on their own, at which point they areÂ not replaced automatically. Running Tasks directly is ideal for short running jobs, perhaps as an example things that were accomplished via CRON.</p>

<p><strong>AÂ service</strong>Â is used to guarantee that you always have some number of TasksÂ running at all times. If a Taskâ€™s container exits due to error, or the underlying EC2 instance fails and is replaced, the ECS Service will replace the failed Task. This is why we createÂ ClustersÂ so that the Service has plenty of resources in terms of CPU, Memory and Network ports to use. To us it doesnâ€™t really matter which instance Tasks run on so long as they run. A Service configurationÂ referencesÂ a Task definition. A Service is responsible forÂ creating Tasks.</p>

<p>Services are typically used for long running applications like web servers. For example, if I deployed my website powered by Node.JS in Oregon (us-west-2) I would want say at least three Tasks running across the three Availability Zones (AZ) for the sake of High-Availability; if one fails I have another two and the failed one will be replaced (read that asÂ self-healing!). Creating a Service is the way to do this. If I had 6 EC2 instances in my cluster, 2 per AZ, the Service will automatically balance Tasks across zones as best it can while also considering cpu, memory, and network resources.</p>

<p>**Docker compose **allows an application developer to define their application stack as a set of distributed applications. Compose allows it work just the same whatever infrastructure it runs on. Decoupling the infrastructure from the application significantly increases the portability of the distributed application.Â 
A service definition contains configuration that is applied to each container started for that service, much like passing command-line parameters toÂ docker container createÂ Likewise, network and volume definitions are analogous toÂ docker network createÂ andÂ docker volume createÂ As withÂ docker container create, options specified in the Dockerfile, such asÂ CMD,Â EXPOSE,Â VOLUME,Â ENV, are respected by default - you donâ€™t need to specify them again inÂ docker-compose.yml</p>

<p>UPDATE:
Iâ€™m not sure it helps to think of these things hierarchically.
Another very important point is that a Service can be configured to use a load balancer, so that as it creates the Tasksâ€”that is it launches containers defined in the Task Defintionâ€”the Service will automatically register the containerâ€™s EC2 instance with the load balancer. Tasks cannot be configured to use a load balancer, only Services can.</p>

<hr />

<h1 id="useful-reads">Useful Reads</h1>

<p><strong>About itd</strong></p>

<p><a href="https://stackoverflow.com/questions/30137135/confused-about-docker-t-option-to-allocate-a-pseudo-tty">Docker -it flags</a></p>

<ul>
  <li>
    <p>-iÂ (interactive) is about whether to keep stdin open (some programs, like bash, use stdin and other programs donâ€™t).Â </p>
  </li>
  <li>
    <p>-dÂ (detached) is about whether theÂ docker runÂ command waits for the process being run to exit. Thus, they are orthogornal and not inherently contradictory. A program like bash exits when stdin in closed, so withoutÂ -i, it exits immediately.</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>-tÂ allocates a pseudo-tty. You can see the difference from running bash withÂ -itÂ vs with justÂ -i. For example, without -t, you donâ€™t get any prompt andÂ lsÂ shows results in one column. This difference is like the difference between runningÂ lsÂ and runningÂ ls</td>
          <td>cat, whereÂ catÂ does not have a pseudo-tty.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>When youÂ docker runÂ bash in a container,Â -itÂ andÂ -itdÂ behave differently as follows:</li>
  <li>WithÂ -it,Â docker runÂ gives you the bash prompt immediately.</li>
  <li>WithÂ -itd,Â docker runÂ exits immediately, but you canÂ docker attachÂ after that and get the bash prompt just as if you had just doneÂ docker run -it.</li>
</ul>

<p><strong>Using Exec in entry-point scripts</strong></p>

<p>exec â€œ$@â€Â is typically used to make the entrypoint a pass through that then runs the docker command.</p>

<p>exec will replace the parent process, rather than have two processes running. Â Example, At the exec lineÂ entrypoint.sh, the shell running as pid 1 will replace itself with the commandÂ server start. Â For example, if Redis was started without exec, it will not receive aÂ SIGTERMÂ uponÂ docker stopÂ and will not get a chance to shutdown cleanly. In some cases, this can lead to data loss or zombie processes. Example: This is critical for signal handling. Without usingÂ exec, theÂ server startÂ in the above example would run as another pid, and after it exits, you would return to your shell script. With a shell in pid 1, a SIGTERM will be ignored by default. That means the graceful stop signal thatÂ docker stopÂ sends to your container, would never be received by theÂ serverprocess. After 10 seconds (by default),Â docker stopÂ would give up on the graceful shutdown and send a SIGKILL that will force your app to exit, but with potential data loss or closed network connections, that app developers could have coded around if they received the signal. It also means your container will always take the 10 seconds to stop. If you do start child processes (i.e. donâ€™t use exec), the parent process becomes responsible for handling and forwarding signals as appropriate. This is one of the reasons itâ€™s best to use supervisord or similar when running multiple processes in a container, as it will forward signals appropriately.</p>

<p>without exec</p>
<ul>
  <li>parent shell starts</li>
  <li>parent shell forks child
    <ul>
      <li>child runs</li>
      <li>child exits</li>
    </ul>
  </li>
  <li>parent shell exits
with exec</li>
  <li>parent shell starts</li>
  <li>parent shell forks child, replaces itself with child</li>
  <li>child program runs taking over the shellâ€™s process</li>
  <li>child exits</li>
</ul>

<p>After all, at the end of the day a container is a set of virtualized processes.</p>

<p>In docker you should only execute one process per container because if you donâ€™t, the process that forked and went background is not monitored and may stop without you knowing it.
When you useÂ <code class="highlighter-rouge">CMD cron &amp;&amp; tail -f /var/log/cron.log</code>Â the cron process basically fork in order to executeÂ cronÂ in background, the main process exits and let you executeÂ tailfÂ in foreground. The background cron process could stop or fail you wonâ€™t notice, your container will still run silently and your orchestration tool will not restart it.
You can avoid such a thing by redirecting directly the cronâ€™s commands output into your dockerÂ stdoutÂ andÂ stderrÂ which are located respectively inÂ <code class="highlighter-rouge">/proc/1/fd/1</code>Â andÂ <code class="highlighter-rouge">/proc/1/fd/2</code> (The adopted solutionÂ may be dangerous in a production environment)
Using basic shell redirects you may want to do something like this :</p>

<p><strong>You can use Docker Remote API to extract the log messages or run commands remotely</strong></p>

<p>To enable the Docker Remote API, add the following line to the file /etc/default/docker:
DOCKEROPTS=â€™-H tcp://0.0.0.0:4243 -H unix:///var/run/docker.sockâ€™</p>

<p>When Docker is restarted, it listens for HTTP API requests on port 4243 (you can specify a different port) and also on a socket. To get all the messages, you can issue the GET request:
http://<docker host="">:4243/containers/<container name="">/logs?stdout=1&amp;stderr=1</container></docker></p>

<p>For running a command inside a docker, remotely, edit the file <code class="highlighter-rouge">/lib/systemd/system/docker.service</code></p>

<p>Modify the line that starts with ExecStart to look like this:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ExecStart=/usr/bin/docker daemon -H fd:// -H tcp://0.0.0.0:4243
</code></pre></div></div>
<p>Where the addition is the <code class="highlighter-rouge">â€œ-H tcp://0.0.0.0:4243â€</code> part.</p>

<p>Make sure the Docker service notices the modified configuration:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>systemctl daemon-reload
</code></pre></div></div>
<p>Restart the Docker service:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sudo service docker restart
</code></pre></div></div>

<p>Test that the Docker API is indeed accessible:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl http://localhost:4243/version
</code></pre></div></div>

<p><a href="https://stackoverflow.com/questions/32878795/run-command-inside-of-docker-container-using-ansible/41626257#41626257">Reference</a></p>

:ET