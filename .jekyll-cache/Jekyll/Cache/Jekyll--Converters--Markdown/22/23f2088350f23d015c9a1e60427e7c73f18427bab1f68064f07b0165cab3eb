I"C≠<p>RL methods follow two general ideas:</p>
<ul>
  <li>Value Based: We could estimate how ‚Äúgood‚Äù a state or state/action pair is, and then we could have our agent behave greedily.</li>
  <li>Policy Based: We could find an optimal policy, a policy being a function that outputs an action for each state. 
These model and optimize the policy directly (rather than indirectly). The value of the reward (objective) function depends on this policy and then various algorithms can be applied to optimize for the best reward.</li>
</ul>

<hr />

<p>Policy based methods are better in the following settings:</p>
<ul>
  <li>There are situations where it is more efficient to represent policy than value functions. If the state space is continuous or too large, it would be difficult to store value functions.  In continuous action spaces* (e.g., robot control, trying to show the perfect advert): Value-based methods require finding the maximum, which can be prohibitively expensive.</li>
  <li>State aliasing: If you are only partially observing the environment, or your features limit your view of the world, then it can be optimal to use a stochastic policy, in which case policy search methods can be better than value methods.</li>
  <li>Value-based methods can oscillate or diverge. With policy gradients, you smoothly update your policy. You don‚Äôt get dramatic changes in what action you are going to take. You make incremental changes to policy in the direction that makes policy better.</li>
</ul>

<hr />
<p><strong>Policy Gradient Theorem</strong></p>

<p>The objective in RL is to maximize the expected return over trajectories:</p>

\[J(\theta) = E_{\tau \sim \pi_{\theta}} [R(\tau)]\]

<p>Where a trajectory \(\tau = (s_0, a_0, s_1, a_1, ..., s_T, a_T)\).</p>

<p>We would like to find the parameters that maximize this, and for that we would like to find the gradient of the function with respect to \(\theta\).  But we can‚Äôt find the gradient here, because the function involves non-differentiable computation.</p>

<p>Often we know the gradient of the policy, because we can chose it so that it is differentiable ( (Gaussian, softmax, differentiable neural network etc.) Through a neat trick, called the policy gradient theorem pulls the gradient outside the sampling process.</p>

\[\nabla_{\theta} J(\theta) = E_{\tau \sim \pi_{\theta}} [R(\tau) \cdot \nabla_{\theta} \log \pi_{\theta}(\tau)]\]

<p>Factorizing the Trajectory Probability:</p>

\[\pi_{\theta}(\tau) = p(s_0) \prod_{t=0}^{T} \pi_{\theta}(a_t | s_t) p(s_{t+1} | s_t, a_t)\]

\[\nabla_{\theta} J(\theta) = E_{\tau} [R(\tau) \cdot \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t)]\]

<p>The per-step sum form:</p>

<p>\(R(\tau) \cdot \sum_{t} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t)\)
is computationally what we actually implement, because we compute the log-prob of each action as we sample the trajectory.</p>

<p><strong>Note</strong>
Credit assignment:</p>

<p>Instead of using the same R(œÑ) for all timesteps, we can refine credit assignment by using the return from each step onward:</p>

\[R_t = Œ£_{k=t}^{T} Œ≥^{k‚àít} * r_k\]

<p>Then the gradient becomes:</p>

\[‚àá_Œ∏ J(Œ∏) = E_{œÑ‚àºœÄ_Œ∏} [ Œ£_{t=0}^{T} R_t * ‚àá_Œ∏ log œÄ_Œ∏(a_t | s_t) ]\]

<p>The trajectory return  \($R(œÑ)\) and stepwise return  forms are mathematically equivalent in expectation</p>

<p>Using full  \(ùëÖ(ùúè)\) gives the same weight to every action that can high variance in long episodes. Using stepwise  gives more precise credit assignment to actions closer to the rewards. This is variance reduction but keeps the estimator unbiased.</p>

<p>The <strong>log trick</strong> (likelihood ratio trick): \(\nabla_{\theta} \pi_{\theta} = \pi_{\theta} \cdot \nabla_{\theta} \log \pi_{\theta}\) makes the math work cleanly with sampling.</p>

<p>We want to optimize expected return:</p>

\[\nabla_{\theta} J(\theta) = \nabla_{\theta} \mathbb{E}_{\tau} [R(\tau)] = \mathbb{E}_{\tau} \left[ R(\tau) \nabla_{\theta} \log p_{\theta}(\tau) \right]\]

<p>This pulls the gradient outside the sampling process.</p>

<p>The trajectory probability is a product of policy probabilities over time, the log of a product becomes a sum of logs. Only the policy depends on $\theta$, so we ignore the rest when computing gradients.</p>

\[\nabla_{\theta} J(\theta) = \mathbb{E}_{\tau} \left[ \sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \cdot R(\tau) \right]\]

<hr />

<p>In the standard Reinforcement Learning (RL) process, the following steps occur:</p>

<p>The state at time \(t\), denoted as \(s_t\), is processed through the policy, denoted as \(œÄ_Œ∏\), to determine the action at time \(t\), denoted as \(a_t\). This action is then introduced to the environment, which in turn provides the new state at time \(t+1\), denoted as \(s_{t+1}\), and the reward at time \(t\), denoted as \(r_t\).</p>

<p>This can be mathematically represented as:</p>

\[s_t ‚Üí œÄ_Œ∏(a_t) ‚Üí Environment ‚Üí s_{t+1}, r_t\]

<p>Obtaining a reward in a trajectory in RL involves non-differentiable computation, even the policy is differentiable:</p>

<ul>
  <li>Action is sampled (discrete or stochastic). There is no gradient through sampling.</li>
  <li>The transition in the environment \(s_t, a_t ‚Ü¶ s_{t+1}\) is a black box, implying that we do not have knowledge how the environment selects state based on action.</li>
  <li>Similarly, the reward \(r_t = R(s_t, a_t)\) is also a black box, indicating that we do not have knowledge of the reward that the environment provides based on our action and the state. We cannot compute \(‚àÇR/‚àÇa\)</li>
</ul>

<p>We are unable to apply the chain rule from the reward back to \(Œ∏\) in a continuous manner because we cannot determine how the sampling or environmental reward changes with \(Œ∏\).</p>

<p>As a result, we consider the reward as an external scalar signal that influences a policy gradient.</p>

<hr />

<p>What if we could model the environment and reward function as differentiable?</p>

<p>If we could model the environment and reward function as differentiable (as in differentiable simulators)</p>

<p>\(f: s_{t+1} = f(s_t, a_t)\)
\(s_{t+1}\) is differentiable</p>

\[r_t = g(s_t, a_t)\]

<p>\(r_t\) is differentiable</p>

<p>Then you could skip policy gradient and just do end-to-end backpropagation:</p>

\[\nabla_{\theta} R = \sum_{t} \frac{\partial R}{\partial a_t} \frac{\partial a_t}{\partial \theta}\]

<p>This is much more sample-efficient because you use true gradients instead of REINFORCE‚Äôs noisy estimates.</p>

<hr />

<p><strong>Comparison with supervised learning</strong></p>

<p>Cross-entropy loss is really the negative log-likelihood of the correct label
Gradient of supervised learning is:</p>

\[\nabla_{\theta} L = -\nabla_{\theta} \log p_{\theta}(y|x)\]

<p>In supervised learning, we know the target label ‚Üí smooth differentiable loss.
In RL, the ‚Äúlabel‚Äù is implicit and stochastic via reward ‚Üí need expectation over trajectories</p>

<hr />

<h2 id="softmax-policy-for-discrete-actions">Softmax Policy for Discrete Actions</h2>

<p>Alternative to \(\epsilon\)-greedy:</p>

\[\pi_i = \frac{e^{z_i}}{\sum_j e^{z_j}}\]

\[\log(\pi_i) = z_i - \log\left(\sum_j e^{z_j}\right)\]

<p>The gradient with respect to each logit \(z_k\) is:</p>

\[\frac{\partial \log(\pi_i)}{\partial z_k} = 
\begin{cases} 
1 - \pi_i &amp; \text{if } k = i \\
-\pi_k &amp; \text{if } k \neq i 
\end{cases}\]

<p>In a linear softmax policy, logits are:</p>

\[z_a(s) = \theta_a^T s\]

\[\log(\pi_{\theta}(i|s)) = \theta_i^T s - \log\left(\sum_b e^{\theta_b^T s}\right)\]

<p>Derivative w.r.t parameters \(\theta_k\):</p>

\[\frac{\partial \log(\pi_{\theta}(i|s))}{\partial \theta_k} = 
\begin{cases} 
s \cdot (1 - \pi_{\theta}(i|s)), &amp; \text{if } k = i \\
-s \cdot \pi_{\theta}(k|s), &amp; \text{if } k \neq i 
\end{cases}\]

<p>For a single step with return \(G_t\) and learning rate \(\alpha\):</p>

<p>The update rule for the parameters \(\theta_k\) is given by:</p>

\[\theta_k \leftarrow \theta_k + \alpha \cdot G_t \cdot \nabla_{\theta_k} \log(\pi_{\theta}(a_t | s_t))\]

<p>This implies:</p>

<p>If the chosen action is \(k = a_t\):</p>

\[\theta_{a_t} \leftarrow \theta_{a_t} + \alpha \cdot G_t \cdot (1 - \pi_{\theta}(a_t | s_t)) \cdot s_t\]

<p>For other actions:</p>

\[\theta_k \leftarrow \theta_k - \alpha \cdot G_t \cdot \pi_{\theta}(k | s_t) \cdot s_t\]

<p>If a feature occurs and gets more reward, we want to adjust the policy to have more of that feature.</p>

<p><strong>Example</strong>: Gaussian policy for continuous actions.</p>

<hr />

<p>Why use a neural network (limitations if a linear model)
A single flat hyperplane divides state space into ‚Äúprefer action i‚Äù vs. ‚Äúprefer action j‚Äù If the environment is simple and separable, this works. But it cannot capture complex, non-linear decision regions
First layer warps the state space into a feature space. In feature space, the final decision is still linear ‚Üí hyperplanes in feature space When mapped back to the original state space, those hyperplanes become curved, flexible surfaces.</p>

<hr />

<p>Episode-based update (REINFORCE): High variance (needs normalization or baseline). Learning is slow if episodes are long</p>

<ul>
  <li>\(G_t\) is a single noisy Monte Carlo sample of the return.  Especially early on, a single high-return trajectory can push the policy too far, hurting generalization. Lucky or unlucky episodes can cause big policy swings. Risk of catastrophic forgetting caused by over-updating from a single trajectory.</li>
</ul>

<p>We can reduce variance by subtracting a baseline:</p>

<p>Instead of \(G_t\), use:</p>

\[A_t = G_t - b(s_t)\]

<p>\(b(s_t)\) is a baseline (\(V(s_t)\) in Actor-Critic)</p>

<p>All modern policy gradient methods (PPO, A2C, A3C, SAC) include: A critic, Advantage normalization, Or mini-batch updates
Step-based update (Actor-Critic):  Reduces variance because only deviations from expected return drive updates.  Smaller, more targeted updates, Less destabilizing global shifts in policy weights.  Lower variance, More sample-efficient.
A2C/A3C were breakthrough algorithms for: Continuous learning in long episodes</p>

<hr />

<h2 id="implementation">Implementation</h2>

<p>We define the policy neural network via Torch‚Äôs neural network module.</p>

<p><strong>Cartpole</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PolicyNetwork</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p><strong>Pong</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PolicyNetwork</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">6400</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">leaky_relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="c1"># self.dropout = nn.Dropout(dropout)
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div>

<p>Given a trajectory of rewards, the folllowing function gives you discounted, normalized returns per step, ready for use in policy gradient loss calculation:
\(G_t = r_t + Œ≥*r_{t+1} + Œ≥^2*r_{t+2}\)</p>

<p><strong>Cartpole</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_stepwise_returns</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">discount_factor</span><span class="p">):</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">rewards</span><span class="p">):</span>
         <span class="c1">#  if r != 0: R = 0 reset the sum; game boundary (pong)
</span>        <span class="n">R</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">R</span> <span class="o">*</span> <span class="n">discount_factor</span> 
        <span class="n">returns</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>
    <span class="n">normalized_returns</span> <span class="o">=</span> <span class="p">(</span><span class="n">returns</span> <span class="o">-</span> <span class="n">returns</span><span class="p">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">returns</span><span class="p">.</span><span class="n">std</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">normalized_returns</span>
</code></pre></div></div>
<p><strong>Example:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rewards</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">discount_factor</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="c1"># Raw returns: [2.458, 1.62, 1.8, 2]
# Discounted normalized returns: tensor([ 1.57, -1.13, -0.55,  0.11])
</span></code></pre></div></div>

<p>We can improve on this:</p>

<hr />

<p><strong>Forward Pass for Cartpole</strong></p>

<p>In a forward pass, we collect an observation, run it through the stochastic policy that outputs a distribution over actions, then sample an action, collect the log probability of action and the reward, then use the collected rewards to find discounted normalized returns per step.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward_pass</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">discount_factor</span><span class="p">):</span>
    <span class="n">log_prob_actions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">episode_return</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">policy</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">observation</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">observation</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">observation</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">action_pred</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
        <span class="n">action_prob</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">action_pred</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">distributions</span><span class="p">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">action_prob</span><span class="p">)</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="n">log_prob_action</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span>
        <span class="n">log_prob_actions</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_prob_action</span><span class="p">)</span>
        <span class="n">rewards</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
        <span class="n">episode_return</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">log_prob_actions</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">log_prob_actions</span><span class="p">)</span>
    <span class="n">stepwise_returns</span> <span class="o">=</span> <span class="n">calculate_stepwise_returns</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">discount_factor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">episode_return</span><span class="p">,</span> <span class="n">stepwise_returns</span><span class="p">,</span> <span class="n">log_prob_actions</span>
</code></pre></div></div>

<p>For a binary action with sigmoid output \(p\), the log-likelihood of the policy is given by:</p>

\[\log(\pi(a|s)) = 
\begin{cases} 
\log(p) &amp; \text{if action = 1} \\
\log(1 - p) &amp; \text{if action = 0}
\end{cases}\]

<p>The Binary Cross-Entropy (BCE) loss function is defined as:</p>

<p>BCE(p, y) = -[y * log(p) + (1 - y) * log(1 - p)]</p>

<p>This can be broken down into two cases:</p>

<ol>
  <li>If y = 1, the BCE loss becomes -log(p)</li>
  <li>If y = 0, the BCE loss becomes -log(1 - p)</li>
</ol>

<p>Interestingly, the BCE loss is equivalent to the negative log-likelihood for a Bernoulli distribution. 
This is exactly the same as the negative log probability (-log_prob) used in the REINFORCE algorithm.</p>

<p><strong>Forward Pass for Pong</strong></p>

<p>For Pong, we preprocess the input</p>

<p>Then</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># preprocess the observation, set input to network to be difference image
</span><span class="n">current_observation</span> <span class="o">=</span> <span class="n">image_preprocess</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">input_observation</span> <span class="o">=</span> <span class="n">cur_observation</span> <span class="o">-</span> <span class="n">prev_observation</span> <span class="k">if</span> <span class="n">prev_observation</span>  <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">D</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">prev_observation</span>  <span class="o">=</span> <span class="n">cur_observation</span> 
</code></pre></div></div>

<p>get the output (action probabilities), sample the action, collect the reward.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">action_prob</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_observation</span><span class="p">)</span>
<span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="n">action_prob</span><span class="p">)</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span>
 <span class="c1"># action = 1 if np.random.uniform() &lt; output.item() else 0  roll the dice!    
</span> <span class="c1"># record value
</span><span class="n">observations</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">input_observation</span> <span class="p">)</span>  <span class="c1"># observation
</span><span class="n">gt_labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>  <span class="c1"># label
</span> <span class="c1"># step the environment and get new measurements
</span><span class="n">observation</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
<span class="n">cumulative_return</span> <span class="o">+=</span> <span class="n">reward</span>
<span class="n">rewards</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
<span class="k">if</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span><span class="p">:</span>  <span class="c1"># an episode finished, multiple played games
</span>        <span class="n">episode_number</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">batch_mean_reward</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_sum</span><span class="p">)</span>
<span class="c1"># stack inputs, targets and rewards into a batch
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">observations</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">stepwise_returns</span> <span class="o">=</span> <span class="n">calculate_stepwise_returns</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">discount_factor</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">gt_labels</span><span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Calculating loss and backpropagation. That is the core learning step.</strong></p>

<p>The Policy Gradient theorem tells us that to maximize the expected reward, we need to pull our parameters in the direction that increases the probability of good actions, and decreases the probability of bad actions.</p>

<p>Once we have stepwise returns and log probability of actions, we can calculate loss:</p>

\[‚àá_Œ∏ J(Œ∏) = E_{œÑ‚àºœÄ_Œ∏} [ Œ£_{t=0}^{T} R_t * ‚àá_Œ∏ log œÄ_Œ∏(a_t | s_t) ]\]

<p><strong>Cartpole</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="n">stepwise_returns</span><span class="p">,</span> <span class="n">log_prob_actions</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">stepwise_returns</span> <span class="o">*</span> <span class="n">log_prob_actions</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="k">def</span> <span class="nf">update_policy</span><span class="p">(</span><span class="n">stepwise_returns</span><span class="p">,</span> <span class="n">log_prob_actions</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">stepwise_returns</span> <span class="o">=</span> <span class="n">stepwise_returns</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">calculate_loss</span><span class="p">(</span><span class="n">stepwise_returns</span><span class="p">,</span> <span class="n">log_prob_actions</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
</code></pre></div></div>

<p>You usually don‚Äôt want to track gradients during the environment interaction. You often recompute log-probs at training time for stability and because you may want to process the whole trajectory as a batch
When you call policy_forward, instead of discarding the raw logits, store them:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">preprocessed_observation</span><span class="p">)</span>
<span class="n">logit_buffer</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>  <span class="c1"># store logits for training
</span></code></pre></div></div>
<p>At training time, use the stored logits instead of calling model(inputs) again. Saves computation. Uses more memory (store one logit per timestep) and may complicate batching.</p>

<p><strong>Pong</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1"># Calculate Loss and Grads
</span><span class="n">action_probs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s">'none'</span><span class="p">)(</span><span class="n">action_probs</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">targets</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span>  <span class="c1"># batch_size == grad accumulation iterations
</span><span class="n">weighted_loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">stepwise_returns</span>
<span class="n">weighted_loss</span> <span class="o">=</span> <span class="n">weighted_loss</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>  <span class="c1"># not average, since each episode is used as a one data sample
</span><span class="n">weighted_loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></div>
<p>The Pong version first calculates a supervised loss and then weights it by rewards, creating a policy gradient with a supervised-like loss. The supervised loss at time t, denoted as supervised_loss_t, is given by the cross entropy (BCE) of the model predictions at time t and the action at time t. This can be represented mathematically as:</p>

\[supervised\_loss_t = BCE(actionprobs_t, a_t) = -log(\pi_\theta(a_t | s_t))\]

<p>Where:
\(\pi_\theta(a_t | s_t)\) is the probability of action \(a_t\) given state \(s_t\) under policy \(\theta\).</p>

<p>The loss per timestep \(L_t\) is given by the product of the reward-to-go \(G_t\) and the negative log probability of the action given the state. This can be represented mathematically as:</p>

\[L_t = G_t * (-log(\pi_\theta(a_t | s_t)))\]

<p>Where:</p>
<ul>
  <li>\(G_t\) is the reward-to-go (discounted return) at time t.</li>
</ul>

<p>batch_size represents number of episodes per update. If you divide by batch_size, each episode contributes a smaller gradient, effectively an average gradient, just a scaling to keep gradient magnitudes reasonable. equivalent to using a smaller learning rate.</p>

<p>We could also do as in Cartpole:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">model_preds</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">log_prob_actions</span> <span class="o">=</span> <span class="n">log_probs</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">targets</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">discounted_r</span> <span class="o">*</span> <span class="n">log_prob_actions</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></div>

<p>For the main training loop, you define your hyperparameters:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">MAX_EPOCHS</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="n">DISCOUNT_FACTOR</span> <span class="o">=</span> <span class="mf">0.99</span>
    <span class="n">N_TRIALS</span> <span class="o">=</span> <span class="mi">25</span>
    <span class="n">REWARD_THRESHOLD</span> <span class="o">=</span> <span class="mi">475</span>
    <span class="n">PRINT_INTERVAL</span> <span class="o">=</span> <span class="mi">10</span>
</code></pre></div></div>

<p>Then you construct your network</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1"># Defines network architecture parameters based on environment‚Äôs state and action spaces.
</span>    <span class="n">INPUT_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># Input dimension (state size)
</span>    <span class="n">HIDDEN_DIM</span> <span class="o">=</span> <span class="mi">128</span> <span class="c1"># Number of neurons in hidden layer
</span>    <span class="n">OUTPUT_DIM</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span> <span class="c1"># Number of actions (output dim)
</span>    <span class="n">DROPOUT</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># Dropout probability in policy network
</span>    <span class="n">policy</span> <span class="o">=</span> <span class="n">PolicyNetwork</span><span class="p">(</span><span class="n">INPUT_DIM</span><span class="p">,</span> <span class="n">HIDDEN_DIM</span><span class="p">,</span> <span class="n">OUTPUT_DIM</span><span class="p">,</span> <span class="n">DROPOUT</span><span class="p">)</span>
</code></pre></div></div>

<p>And then you run a loop that does (1) forward passes and (2) parameter updates.
Each forward pass can correspond to one episode. 
When the mean return for episodes is good enough, you can consider the agent trained.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    <span class="n">episode_returns</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">LEARNING_RATE</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">MAX_EPOCHS</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">episode_return</span><span class="p">,</span> <span class="n">stepwise_returns</span><span class="p">,</span> <span class="n">log_prob_actions</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">DISCOUNT_FACTOR</span><span class="p">)</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">update_policy</span><span class="p">(</span><span class="n">stepwise_returns</span><span class="p">,</span> <span class="n">log_prob_actions</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
        <span class="n">episode_returns</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_return</span><span class="p">)</span>
        <span class="n">mean_episode_return</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">episode_returns</span><span class="p">[</span><span class="o">-</span><span class="n">N_TRIALS</span><span class="p">:])</span>
        <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="n">PRINT_INTERVAL</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'| Episode: </span><span class="si">{</span><span class="n">episode</span><span class="p">:</span><span class="mi">3</span><span class="si">}</span><span class="s"> | Mean Rewards: </span><span class="si">{</span><span class="n">mean_episode_return</span><span class="p">:</span><span class="mf">5.1</span><span class="n">f</span><span class="si">}</span><span class="s"> |'</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mean_episode_return</span> <span class="o">&gt;=</span> <span class="n">REWARD_THRESHOLD</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Reached reward threshold in </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s"> episodes'</span><span class="p">)</span>
            <span class="k">break</span>

</code></pre></div></div>

<hr />

<p>Resources:</p>

<ul>
  <li><a href="https://karpathy.github.io/2016/05/31/rl/">Pong by Karpathy</a></li>
  <li><a href="https://www.datacamp.com/tutorial/reinforcement-learning-with-gymnasium">CartPole</a></li>
  <li><a href="https://spinningup.openai.com/en/latest/spinningup/exercises.html#problem-set-1-basics-of-implementation">OpenAI Problem Sets</a></li>
</ul>

<hr />

:ET