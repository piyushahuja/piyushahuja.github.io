I"<p>In some settings, it makes sense to have an agent follow a â€œpolicyâ€ that outputs an action given a state. 
For example, if the state space is continuous or too large, it would be difficult to store value functions.</p>

<p>Thereâ€™s always a deterministic policy for any MDP</p>

<p>State aliasing, If you are only partially observing the environment, or your features limit your view o fthe world, then it can be optimal to use a stochastic policy, in which case policy search methods can be better than value methods.</p>

<p>How do we improve the policy during training?</p>

<p>One way is a class of methods called â€œPolicy gradientâ€ (as opposed to PPO).</p>

<p>These model and optimize the policy directly (rather than indirectly). The policy is usually modeled with a parameterized function respect to /theta. There are situations where it is more efficient to represent policy than value functions.</p>

<p>The value of the reward (objective) function depends on this policy and then various algorithms can be applied to optimize 
 for the best reward.</p>

<hr />

<p>Benefits</p>

<ul>
  <li>
    <p>Better convergence properties. Value based methods can oscillate or diverge. With policy gradients, you smoothly update your policy. You dont get dramatic changes in what action you are going to take. You make incremental changes to policy in the direction what makes policy better</p>
  </li>
  <li>
    <p>Effective in continous actions spaces (e.g. robot, trying to show the perfect advert). Value based method: one has to find out how to find a max. Maximisation can be prohibitally expensive.</p>
  </li>
</ul>

<hr />

<p><strong>Why does gradient of log of policy appears, not the policy itself?</strong></p>

<p>The log trick/ likelihood ratio trick: gradient policy = policy * gradient of log of policy,  makes the math work cleanly with sampling:</p>

<p>Assume:</p>
<ul>
  <li>(1) The policy is differentiable  (doesnt have to be differentiable everywhere, but only when it is actually picking actions, i.e. when it is non zero)</li>
  <li>(2) We know the gradient because e.g. we created our policy (gaussian, softmax  â€” alternatively to e-greedy)</li>
</ul>

<p>We want to optimize expected return:</p>

<p>This pulls the gradient outside the sampling process</p>

<p>The trajectory probability is a product of policy probabilities over time, the log of a product becomes a sum of logs, Only the policy depends on Î¸\thetaÎ¸, so we ignore the rest when computing gradients.</p>

\[\nabla_{\theta} J(\theta) = E_{\tau} \left[ \sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \cdot R(\tau) \right]\]

<hr />

<p>Softmax policy for discrete actions (alternative to epsilon greedy)</p>

\[Ï€_i = e^{z_i} / âˆ‘_j e^{z_j}\]

\[log(Ï€_i) = z_i - log(âˆ‘_j e^{z_j})\]

<p>The gradient with respect to each logit \(z_k\) is:</p>

\[\frac{\partial \log(\pi_i)}{\partial z_k} = 
\begin{cases} 
1 - \pi_i &amp; \text{if } k = i \\
-\pi_k &amp; \text{if } k \neq i 
\end{cases}\]

<p>In a linear softmax policy, logits are:</p>

<p>\(z_a(s) = Î¸_a^T * s\) (Weight actions using linear combination of features)</p>

\[log(Ï€_Î¸(i|s)) = Î¸_i^T * s - log(âˆ‘_b e^(Î¸_b^T * s))\]

<p>Derivative w.r.t parameters Î¸_k:</p>

\[\frac{\partial \log(\pi_{\theta}(i|s))}{\partial \theta_k} = 
\begin{cases} 
s * (1 - \pi_{\theta}(i|s)), &amp; \text{if } k = i \\
-s * \pi_{\theta}(k|s), &amp; \text{if } k \neq i 
\end{cases}\]

<p>For a single step with return \(G_t\) and learning rate \(\alpha\):</p>

<p>The update rule for the parameters \(\theta_k\) is given by:</p>

\[\theta_k â† \theta_k + \alpha * G_t * \nabla_{\theta_k} \log(\pi_{\theta}(a_t | s_t))\]

<p>This implies:</p>

<p>If the chosen action is \(k = a_t\):</p>

\[\theta_{a_t} â† \theta_{a_t} + \alpha * G_t * (1 - \pi_{\theta}(a_t | s_t)) * s_t\]

<p>For other actions:</p>

\[\theta_k â† \theta_k - \alpha * G_t * \pi_{\theta}(k | s_t) * s_t\]

<p>If a feature occurs and gets more reward, we want to adjust the policy to have more of that feature.</p>

<p>Example: Gausian policy. Continuous action.</p>

<hr />

<p>The function gives you discounted, normalized returns per step, ready for use in policy gradient loss calculation.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def calculate_stepwise_returns(rewards, discount_factor):
    returns = []
    R = 0
    for r in reversed(rewards):
        R = r + R * discount_factor
        returns.insert(0, R)
    returns = torch.tensor(returns)
    normalized_returns = (returns - returns.mean()) / returns.std()
    return normalized_returns
</code></pre></div></div>

<p>Example:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rewards = [1, 0, 0, 2]
discount_factor = 0.9
Raw returns: [2.458,1.62,1.8,2]
tensor([ 1.57, -1.13, -0.55,  0.11])
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def calculate_loss(stepwise_returns, log_prob_actions):
    loss = -(stepwise_returns * log_prob_actions).sum()
    return loss
</code></pre></div></div>

<hr />

<p>Linear REINFORCE updates are global in state space</p>

<p>REINFORCE Has High Variance</p>

<p>ssues:</p>

<p>ğº
ğ‘¡
G 
t
â€‹
  is a single noisy Monte Carlo sample of the return</p>

<p>Updates are global â€” changing 
ğœƒ
ğ‘
Î¸ 
a
â€‹
  affects all states in a linear policy</p>

<p>Lucky or unlucky episodes can cause big policy swings</p>

<p>Learning is unstable Especially early on, a single highâ€‘return trajectory can push the policy too far, hurting generalization</p>

<p>We can reduce variance in two main ways:</p>

<p>Baseline Subtraction (Advantage Function)</p>

<p>Instead of 
ğº
ğ‘¡
G 
t
â€‹
 , use:</p>

<p>ğ´
ğ‘¡
=
ğº
ğ‘¡
âˆ’
ğ‘
(
ğ‘ 
ğ‘¡
)
A 
t
â€‹
 =G 
t
â€‹
 âˆ’b(s 
t
â€‹
 )
ğ‘
(
ğ‘ 
ğ‘¡
)
b(s 
t
â€‹
 ) is a baseline (often 
ğ‘‰
(
ğ‘ 
ğ‘¡
)
V(s 
t
â€‹
 ))</p>

<p>Reduces variance because only deviations from expected return drive updates</p>

<p>Intuition:</p>

<p>If an action does as expected, no need to shift policy</p>

<p>If itâ€™s better than expected, increase probability</p>

<p>If worse, decrease probability</p>

<p>b. Actor-Critic</p>

<p>Critic: learns a baseline 
ğ‘‰
ğœ™
(
ğ‘ 
ğ‘¡
)
V 
Ï•
â€‹
 (s 
t
â€‹
 ) (value function)</p>

<p>Actor: updates using advantage 
ğ´
ğ‘¡
=
ğº
ğ‘¡
âˆ’
ğ‘‰
ğœ™
(
ğ‘ 
ğ‘¡
)
A 
t
â€‹
 =G 
t
â€‹
 âˆ’V 
Ï•
â€‹
 (s 
t
â€‹
 )</p>

<p>Effect:</p>

<p>Smaller, more targeted updates</p>

<p>Less destabilizing global shifts in policy weights</p>
:ET