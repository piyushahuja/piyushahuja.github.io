I"Ï0<p>RL methods follow two general ideas:</p>
<ul>
  <li>Value Based: We could estimate how ‚Äúgood‚Äù a state or state/action pair is, and then we could have our agent behave greedily.</li>
  <li>Policy Based: We could find an optimal policy, a policy being a function that outputs an action for each state. 
These model and optimize the policy directly (rather than indirectly). The value of the reward (objective) function depends on this policy and then various algorithms can be applied to optimize for the best reward.</li>
</ul>

<hr />

<p>Policy based methods are better in the following settings:</p>
<ul>
  <li>There are situations where it is more efficient to represent policy than value functions. If the state space is continuous or too large, it would be difficult to store value functions.  In continuous action spaces* (e.g., robot control, trying to show the perfect advert): Value-based methods require finding the maximum, which can be prohibitively expensive.</li>
  <li>State aliasing: If you are only partially observing the environment, or your features limit your view of the world, then it can be optimal to use a stochastic policy, in which case policy search methods can be better than value methods.</li>
  <li>Value-based methods can oscillate or diverge. With policy gradients, you smoothly update your policy. You don‚Äôt get dramatic changes in what action you are going to take. You make incremental changes to policy in the direction that makes policy better.</li>
</ul>

<hr />
<p><strong>Policy Gradient Theorem</strong></p>

<p>The objective in RL is the expected return over trajectories:</p>

\[J(\theta) = E_{\tau \sim \pi_{\theta}} [R(\tau)]\]

<p>Where a trajectory \(\tau = (s_0, a_0, s_1, a_1, ..., s_T, a_T)\).</p>

<p>Policy gradient theorem (REINFORCE) gives:</p>

\[\nabla_{\theta} J(\theta) = E_{\tau \sim \pi_{\theta}} [R(\tau) \cdot \nabla_{\theta} \log \pi_{\theta}(\tau)]\]

<p>Factorizing the Trajectory Probability:</p>

\[\pi_{\theta}(\tau) = p(s_0) \prod_{t=0}^{T} \pi_{\theta}(a_t | s_t) p(s_{t+1} | s_t, a_t)\]

\[\nabla_{\theta} J(\theta) = E_{\tau} [R(\tau) \cdot \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t)]\]

<p>The per-step sum form:</p>

<p>\(R(\tau) \cdot \sum_{t} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t)\)
is computationally what we actually implement, because we compute the log-prob of each action as we sample the trajectory.</p>

<p>The <strong>log trick</strong> (likelihood ratio trick): \(\nabla_{\theta} \pi_{\theta} = \pi_{\theta} \cdot \nabla_{\theta} \log \pi_{\theta}\) makes the math work cleanly with sampling.</p>

<p><strong>Assumptions:</strong></p>
<ol>
  <li>The policy is differentiable</li>
  <li>We know the gradient  (Gaussian, softmax, differentiable neural network etc.)</li>
</ol>

<p>We want to optimize expected return:</p>

\[\nabla_{\theta} J(\theta) = \nabla_{\theta} \mathbb{E}_{\tau} [R(\tau)] = \mathbb{E}_{\tau} \left[ R(\tau) \nabla_{\theta} \log p_{\theta}(\tau) \right]\]

<p>This pulls the gradient outside the sampling process.</p>

<p>The trajectory probability is a product of policy probabilities over time, the log of a product becomes a sum of logs. Only the policy depends on $\theta$, so we ignore the rest when computing gradients.</p>

\[\nabla_{\theta} J(\theta) = \mathbb{E}_{\tau} \left[ \sum_{t=0}^{T-1} \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \cdot R(\tau) \right]\]

<hr />

<p><strong>Non Differentiable Computation</strong></p>

<p>In the standard Reinforcement Learning (RL) process, the following steps occur:</p>

<p>The state at time \(t\), denoted as \(s_t\), is processed through the policy, denoted as \(œÄ_Œ∏\), to determine the action at time \(t\), denoted as \(a_t\). This action is then introduced to the environment, which in turn provides the new state at time \(t+1\), denoted as \(s_{t+1}\), and the reward at time \(t\), denoted as \(r_t\).</p>

<p>This can be mathematically represented as:</p>

\[s_t ‚Üí œÄ_Œ∏(a_t) ‚Üí Environment ‚Üí s_{t+1}, r_t\]

<p>Obtaining a reward in a trajectory in RL involves non-differentiable computation, even the policy is differentiable:</p>

<ul>
  <li>Action is sampled (discrete or stochastic). There is no gradient through sampling.</li>
  <li>The transition in the environment \(s_t, a_t ‚Ü¶ s_{t+1}\) is a black box, implying that we do not have knowledge how the environment selects state based on action.</li>
  <li>Similarly, the reward \(r_t = R(s_t, a_t)\) is also a black box, indicating that we do not have knowledge of the reward that the environment provides based on our action and the state. We cannot compute \(‚àÇR/‚àÇa\)</li>
</ul>

<p>We are unable to apply the chain rule from the reward back to \(Œ∏\) in a continuous manner because we cannot determine how the sampling or environmental reward changes with \(Œ∏\).</p>

<p>As a result, we consider the reward as an external scalar signal that influences a policy gradient.</p>

<hr />

<p>What if we could model the environment and reward function as differentiable?</p>

<p>If we could model the environment and reward function as differentiable (as in differentiable simulators)</p>

<p>\(f: s_{t+1} = f(s_t, a_t)\)
\(s_{t+1}\) is differentiable</p>

\[r_t = g(s_t, a_t)\]

<p>\(r_t\) is differentiable</p>

<p>Then you could skip policy gradient and just do end-to-end backpropagation:</p>

\[\nabla_{\theta} R = \sum_{t} \frac{\partial R}{\partial a_t} \frac{\partial a_t}{\partial \theta}\]

<p>This is much more sample-efficient because you use true gradients instead of REINFORCE‚Äôs noisy estimates.</p>

<hr />

<p><strong>Comparison with supervised learning</strong></p>

<p>Cross-entropy loss is really the negative log-likelihood of the correct label
Gradient of supervised learning is:</p>

\[\nabla_{\theta} L = -\nabla_{\theta} \log p_{\theta}(y|x)\]

<p>In supervised learning, we know the target label ‚Üí smooth differentiable loss.
In RL, the ‚Äúlabel‚Äù is implicit and stochastic via reward ‚Üí need expectation over trajectories</p>

<hr />

<h2 id="softmax-policy-for-discrete-actions">Softmax Policy for Discrete Actions</h2>

<p>Alternative to \(\epsilon\)-greedy:</p>

\[\pi_i = \frac{e^{z_i}}{\sum_j e^{z_j}}\]

\[\log(\pi_i) = z_i - \log\left(\sum_j e^{z_j}\right)\]

<p>The gradient with respect to each logit \(z_k\) is:</p>

\[\frac{\partial \log(\pi_i)}{\partial z_k} = 
\begin{cases} 
1 - \pi_i &amp; \text{if } k = i \\
-\pi_k &amp; \text{if } k \neq i 
\end{cases}\]

<p>In a linear softmax policy, logits are:</p>

\[z_a(s) = \theta_a^T s\]

\[\log(\pi_{\theta}(i|s)) = \theta_i^T s - \log\left(\sum_b e^{\theta_b^T s}\right)\]

<p>Derivative w.r.t parameters \(\theta_k\):</p>

\[\frac{\partial \log(\pi_{\theta}(i|s))}{\partial \theta_k} = 
\begin{cases} 
s \cdot (1 - \pi_{\theta}(i|s)), &amp; \text{if } k = i \\
-s \cdot \pi_{\theta}(k|s), &amp; \text{if } k \neq i 
\end{cases}\]

<p>For a single step with return \(G_t\) and learning rate \(\alpha\):</p>

<p>The update rule for the parameters \(\theta_k\) is given by:</p>

\[\theta_k \leftarrow \theta_k + \alpha \cdot G_t \cdot \nabla_{\theta_k} \log(\pi_{\theta}(a_t | s_t))\]

<p>This implies:</p>

<p>If the chosen action is \(k = a_t\):</p>

\[\theta_{a_t} \leftarrow \theta_{a_t} + \alpha \cdot G_t \cdot (1 - \pi_{\theta}(a_t | s_t)) \cdot s_t\]

<p>For other actions:</p>

\[\theta_k \leftarrow \theta_k - \alpha \cdot G_t \cdot \pi_{\theta}(k | s_t) \cdot s_t\]

<p>If a feature occurs and gets more reward, we want to adjust the policy to have more of that feature.</p>

<p><strong>Example</strong>: Gaussian policy for continuous actions.</p>

<hr />

<p>Why use a neural network (limitations if a linear model)
A single flat hyperplane divides state space into ‚Äúprefer action i‚Äù vs. ‚Äúprefer action j‚Äù If the environment is simple and separable, this works. But it cannot capture complex, non-linear decision regions
First layer warps the state space into a feature space. In feature space, the final decision is still linear ‚Üí hyperplanes in feature space When mapped back to the original state space, those hyperplanes become curved, flexible surfaces.</p>

<hr />

<p>Episode-based update (REINFORCE): High variance (needs normalization or baseline). Learning is slow if episodes are long</p>

<ul>
  <li>\(G_t\) is a single noisy Monte Carlo sample of the return.  Especially early on, a single high-return trajectory can push the policy too far, hurting generalization. Lucky or unlucky episodes can cause big policy swings. Risk of catastrophic forgetting caused by over-updating from a single trajectory.</li>
</ul>

<p>We can reduce variance by subtracting a baseline:</p>

<p>Instead of \(G_t\), use:</p>

\[A_t = G_t - b(s_t)\]

<p>\(b(s_t)\) is a baseline (\(V(s_t)\) in Actor-Critic)</p>

<p>All modern policy gradient methods (PPO, A2C, A3C, SAC) include: A critic, Advantage normalization, Or mini-batch updates
Step-based update (Actor-Critic):  Reduces variance because only deviations from expected return drive updates.  Smaller, more targeted updates, Less destabilizing global shifts in policy weights.  Lower variance, More sample-efficient.
A2C/A3C were breakthrough algorithms for: Continuous learning in long episodes</p>

<hr />

<h2 id="implementation">Implementation</h2>

<p>The function gives you discounted, normalized returns per step, ready for use in policy gradient loss calculation:</p>

<p>```python
def calculate_stepwise_returns(rewards, discount_factor):
    returns = []
    R = 0
    for r in reversed(rewards):
        R = r + R * discount_factor 
        returns.insert(0, R)
    returns = torch.tensor(returns)
    normalized_returns = (returns - returns.mean()) / returns.std()
    return normalized_returns</p>

<p>def calculate_loss(stepwise_returns, log_prob_actions):
    loss = -(stepwise_returns * log_prob_actions).sum()
    return loss</p>

<p>def forward_pass(env, policy, discount_factor):
    log_prob_actions = []
    rewards = []
    done = False
    episode_return = 0
    policy.train()
    observation, info = env.reset()
    while not done:
        observation = torch.FloatTensor(observation).unsqueeze(0)
        action_pred = policy(observation)
        action_prob = F.softmax(action_pred, dim = -1)
        dist = distributions.Categorical(action_prob)
        action = dist.sample()
        log_prob_action = dist.log_prob(action)
        observation, reward, terminated, truncated, info = env.step(action.item())
        done = terminated or truncated
        log_prob_actions.append(log_prob_action)
        rewards.append(reward)
        episode_return += reward
    log_prob_actions = torch.cat(log_prob_actions)
    stepwise_returns = calculate_stepwise_returns(rewards, discount_factor)
    return episode_return, stepwise_returns, log_prob_actions</p>

<p>def update_policy(stepwise_returns, log_prob_actions, optimizer):
    stepwise_returns = stepwise_returns.detach()
    loss = calculate_loss(stepwise_returns, log_prob_actions)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>optimizer.zero_grad()
loss.backward()
optimizer.step()
return loss.item()
</code></pre></div></div>

<p>def main():
    MAX_EPOCHS = 500
    DISCOUNT_FACTOR = 0.99
    N_TRIALS = 25
    REWARD_THRESHOLD = 475
    PRINT_INTERVAL = 10
    INPUT_DIM = env.observation_space.shape[0]
    HIDDEN_DIM = 128
    OUTPUT_DIM = env.action_space.n
    DROPOUT = 0.5
    episode_returns = []
    policy = PolicyNetwork(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT)
    LEARNING_RATE = 0.01
    optimizer = optim.Adam(policy.parameters(), lr = LEARNING_RATE)
    for episode in range(1, MAX_EPOCHS+1):
        episode_return, stepwise_returns, log_prob_actions = forward_pass(env, policy, DISCOUNT_FACTOR)
        _ = update_policy(stepwise_returns, log_prob_actions, optimizer)
        episode_returns.append(episode_return)
        mean_episode_return = np.mean(episode_returns[-N_TRIALS:])
        if episode % PRINT_INTERVAL == 0:
            print(f‚Äô| Episode: {episode:3} | Mean Rewards: {mean_episode_return:5.1f} |‚Äô)
        if mean_episode_return &gt;= REWARD_THRESHOLD:
            print(f‚ÄôReached reward threshold in {episode} episodes‚Äô)
            break</p>

<hr />

<p>Resources:</p>

<ul>
  <li><a href="https://karpathy.github.io/2016/05/31/rl/">Karpathy</a></li>
  <li><a href="https://spinningup.openai.com/en/latest/spinningup/exercises.html#problem-set-1-basics-of-implementation">OpenAI</a></li>
</ul>
:ET